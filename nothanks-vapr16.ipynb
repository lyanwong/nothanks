{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3f87da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:04.040031Z",
     "iopub.status.busy": "2025-04-16T03:27:04.039641Z",
     "iopub.status.idle": "2025-04-16T03:27:08.473885Z",
     "shell.execute_reply": "2025-04-16T03:27:08.472974Z"
    },
    "papermill": {
     "duration": 4.441393,
     "end_time": "2025-04-16T03:27:08.475629",
     "exception": false,
     "start_time": "2025-04-16T03:27:04.034236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import time\n",
    "from math import log, sqrt\n",
    "from collections import defaultdict\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import chain\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e980ad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.484477Z",
     "iopub.status.busy": "2025-04-16T03:27:08.483767Z",
     "iopub.status.idle": "2025-04-16T03:27:08.492475Z",
     "shell.execute_reply": "2025-04-16T03:27:08.491660Z"
    },
    "papermill": {
     "duration": 0.014386,
     "end_time": "2025-04-16T03:27:08.493864",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.479478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Progress bar\n",
    "\n",
    "def log_progress(sequence, every=None, size=None, name='Items'):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{name}: {index} / ?'.format(\n",
    "                        name=name,\n",
    "                        index=index\n",
    "                    )\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{name}: {index} / {size}'.format(\n",
    "                        name=name,\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = \"{name}: {index}\".format(\n",
    "            name=name,\n",
    "            index=str(index or '?')\n",
    "        )\n",
    "\n",
    "bar = tqdm # tqdm if in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7715a35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.501976Z",
     "iopub.status.busy": "2025-04-16T03:27:08.501699Z",
     "iopub.status.idle": "2025-04-16T03:27:08.515431Z",
     "shell.execute_reply": "2025-04-16T03:27:08.514766Z"
    },
    "papermill": {
     "duration": 0.019476,
     "end_time": "2025-04-16T03:27:08.516765",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.497289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Network\n",
    "\n",
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self, n_players, hidden_dim):\n",
    "        super(PolicyValueNet, self).__init__()\n",
    "\n",
    "        # CNN Branch for processing matrix M\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(8),  # Batch Norm after Conv\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),  # Batch Norm after Conv\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4)),  # Downsampling to a fixed size\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 4 * 4, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Fully-connected Branch for processing vector b\n",
    "        self.fc_branch = nn.Sequential(\n",
    "            nn.Linear(3, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Main Branch\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
    "            nn.BatchNorm1d(hidden_dim * 2),  # Batch Norm after Linear\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Policy Head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output in range [0, 1]\n",
    "        )\n",
    "\n",
    "        # Value Head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()   # Output n-dim vector in range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, M, b):\n",
    "        M = self.cnn_branch(M)\n",
    "        b = self.fc_branch(b)\n",
    "        x = torch.cat((M, b), dim=-1)\n",
    "        x = self.main_branch(x)\n",
    "        p = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        # print(f\"Debug: Policy Head Output (p): {p}\")  # Debugging output\n",
    "        return p, value\n",
    "\n",
    "def train_nn(model, batch_size, n_players, hidden_dim, input_state, target_policy, target_value):\n",
    "    M, b = input_state\n",
    "    M = torch.tensor(M, dtype=torch.float32)\n",
    "    b = torch.tensor(b, dtype=torch.float32)\n",
    "    target_policy = torch.tensor(target_policy, dtype=torch.float32).view(-1, 1)\n",
    "    target_value = torch.tensor(target_value, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "    # Define optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    weights = torch.where(target_policy == 1., torch.tensor(1.1), torch.tensor(1.0))  # Adjust this value as needed\n",
    "    policy_loss_fn = nn.BCELoss(weight=weights)  # Binary Cross-Entropy Loss with positive weight\n",
    "    value_loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Training loop (few steps)\n",
    "    for step in range(10):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        pred_policy, pred_value = model(M, b)\n",
    "\n",
    "        # Compute policy loss\n",
    "        policy_loss = policy_loss_fn(pred_policy, target_policy)\n",
    "    \n",
    "        # Compute value loss\n",
    "        value_loss = value_loss_fn(pred_value, target_value)\n",
    "\n",
    "        # Compute L2 regularization term (sum of all parameters' squared values)\n",
    "        l2_lambda = 1e-4\n",
    "        l2_penalty = sum(w.pow(2.0).sum() for w in model.parameters())\n",
    "\n",
    "        # Combine losses\n",
    "        loss = policy_loss + value_loss*0.01 + l2_lambda * l2_penalty\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        if step == 9:\n",
    "            print(f\"Loss = {loss.item():.4f}, \"\n",
    "                  f\"Policy Loss = {policy_loss.item():.4f}, \"\n",
    "                  f\"Value Loss = {value_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee71b13e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.525015Z",
     "iopub.status.busy": "2025-04-16T03:27:08.524730Z",
     "iopub.status.idle": "2025-04-16T03:27:08.559497Z",
     "shell.execute_reply": "2025-04-16T03:27:08.558831Z"
    },
    "papermill": {
     "duration": 0.040663,
     "end_time": "2025-04-16T03:27:08.560897",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.520234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Game\n",
    "\n",
    "ACTION_TAKE = 0\n",
    "ACTION_PASS = 1\n",
    "\n",
    "def diff(first, second):\n",
    "    second = set(second)\n",
    "    return [item for item in first if item not in second]\n",
    "\n",
    "@dataclass\n",
    "class NoThanksConfig:\n",
    "    min_card: int = 3\n",
    "    max_card: int = 14\n",
    "    n_omit_cards: int = 3\n",
    "    n_players: int = 3\n",
    "    start_coins: int = 4\n",
    "\n",
    "\n",
    "class NoThanksBoard():\n",
    "    def __init__(self, n_players = 3, config = NoThanksConfig):\n",
    "        self.n_players = n_players\n",
    "        self.min_card = config.min_card\n",
    "        self.max_card = config.max_card\n",
    "        self.full_deck = list(range(self.min_card, self.max_card+1))\n",
    "        self.n_omit_cards = config.n_omit_cards\n",
    "        self.n_cards = self.max_card - self.min_card + 1\n",
    "        self.start_coins = config.start_coins\n",
    "        random.seed(999)\n",
    "\n",
    "            \n",
    "    # state: ((player coins),(player cards),(card in play, coins in play, n_cards_remaining, current player))\n",
    "    def starting_state(self, current_player = 0):\n",
    "        coins = [self.start_coins for i in range(self.n_players)]\n",
    "        cards = [[] for i in range(self.n_players)]\n",
    "\n",
    "        card_in_play = random.choice(self.full_deck)\n",
    "        \n",
    "        coins_in_play = 0\n",
    "        n_cards_in_deck = self.n_cards - 1 - self.n_omit_cards\n",
    "\n",
    "        return coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "\n",
    "        state = self.unpack_state(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        if action == ACTION_TAKE:\n",
    "            cards[current_player].append(card_in_play)\n",
    "            coins[current_player] += coins_in_play\n",
    "\n",
    "            all_player_cards = [card for player_cards in cards for card in player_cards]\n",
    "            cards_in_deck = diff(self.full_deck, all_player_cards)\n",
    "            current_player = current_player\n",
    "            \n",
    "            if cards_in_deck and n_cards_in_deck > 0:   \n",
    "                # random.shuffle(list(cards_in_deck))\n",
    "                card_in_play = random.choice(cards_in_deck)\n",
    "                n_cards_in_deck -= 1\n",
    "            else:\n",
    "                card_in_play = None\n",
    "            coins_in_play = 0\n",
    "\n",
    "        else:\n",
    "            coins[current_player] -= 1\n",
    "            coins_in_play += 1\n",
    "            current_player += 1\n",
    "        \n",
    "        if current_player == self.n_players:\n",
    "            current_player = 0\n",
    "\n",
    "        next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n",
    "        return self.pack_state(next_state)\n",
    "    \n",
    "    def all_possible_next(self, state, action):\n",
    "        if action == ACTION_PASS:\n",
    "            return self.next_state(state, action)\n",
    "        elif action == ACTION_TAKE:\n",
    "            next_states = []\n",
    "            state = self.unpack_state(state)\n",
    "            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "            cards[current_player].append(card_in_play)\n",
    "            coins[current_player] += coins_in_play\n",
    "            n_cards_in_deck -= 1\n",
    "            coins_in_play = 0\n",
    "\n",
    "            all_player_cards = [card for player_cards in cards for card in player_cards]\n",
    "            cards_in_deck = diff(self.full_deck, all_player_cards)\n",
    "            current_player = current_player\n",
    "            \n",
    "            if not cards_in_deck:\n",
    "                return self.next_state(state, action)\n",
    "            else:\n",
    "                for card in cards_in_deck: \n",
    "                    card_in_play = card\n",
    "                    if current_player == self.n_players:\n",
    "                        current_player = 0\n",
    "                    next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n",
    "                    next_state = self.pack_state(next_state)\n",
    "                    next_states.append(next_state)\n",
    "            \n",
    "            return next_states\n",
    "\n",
    "    def is_legal(self, state, action):\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        if card_in_play is None:\n",
    "            return False\n",
    "        if coins[current_player] <= 0 and action == ACTION_PASS:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def legal_actions(self, state):\n",
    "        actions = []\n",
    "        \n",
    "        if self.is_legal(state, ACTION_TAKE):\n",
    "            actions.append(ACTION_TAKE)\n",
    "\n",
    "        if self.is_legal(state, ACTION_PASS):\n",
    "            actions.append(ACTION_PASS)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def pack_state(self, state):\n",
    "        coins, cards, details = state\n",
    "        packed_state = tuple(coins), tuple(map(tuple, cards)), details\n",
    "        return packed_state\n",
    "\n",
    "    def unpack_state(self, packed_state):\n",
    "        coins, cards, details = packed_state\n",
    "        coins = list(coins)\n",
    "        cards = list(map(list, cards))\n",
    "        return coins, cards, details\n",
    "    \n",
    "\n",
    "    def standard_state(self, state):\n",
    "        \"\"\"\n",
    "        Input state (packed or unpacked): ([coins], [[cards]], (card_in_play, coins_in_play, n_cards_in_deck, current_player))\n",
    "        Transform state into the required format:\n",
    "        1. Extract the state into M and b where M is the card/coin matrix and b is the vector (card_in_play, coins_in_play, n_cards_in_deck)\n",
    "        2. Rotate M such that the first row corresponds to the current player\n",
    "        3. Transform M into array of shape (3, n_players, 33)\n",
    "        \"\"\"\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "        \n",
    "        # Step 1: Build the M matrix (n_players x 34)\n",
    "        M = []\n",
    "        for k in range(self.n_players):\n",
    "            # Initialize the card representation for player k\n",
    "            card_rep = [0] * self.n_cards  # 33 cards\n",
    "\n",
    "            # Set 1 for each card player k has\n",
    "            for card in cards[k]:\n",
    "                card_rep[card - self.min_card] = 1  # Cards are indexed from 3 to 35\n",
    "\n",
    "            # Add the number of coins for player k\n",
    "            M.append(card_rep + [coins[k]])  # 33 card columns + 1 coin column\n",
    "        \n",
    "        # Step 2: Build the b vector (card_in_play, coins_in_play, n_cards_in_deck)\n",
    "        b = np.array([card_in_play, coins_in_play, n_cards_in_deck])\n",
    "\n",
    "        # Step 3: Rotate M such that the first row is the current player\n",
    "        M_rotated = M[current_player:] + M[:current_player]  # Rotate the matrix\n",
    "        \n",
    "        # Step 4: Transform M into array of shape (3, n_players, 33)\n",
    "        # where M[0] is the card matrix, M[1] is the coin matrix, M[2] is the card in play\n",
    "        M_transformed = np.zeros((3, self.n_players, self.n_cards))\n",
    "        M_rotated = np.array(M_rotated)\n",
    "        M_transformed[0] = M_rotated[:, :-1]  # Card matrix\n",
    "        M_transformed[1] = np.repeat(M_rotated[:, -1][:, np.newaxis], self.n_cards, axis=1)  # Coin matrix\n",
    "        M_transformed[1] = M_transformed[1] / (self.start_coins * self.n_players)  # Normalize coins to be between 0 and 1\n",
    "        \n",
    "        card_in_play_onehot = np.zeros(self.n_cards)\n",
    "        if self.min_card <= card_in_play <= self.max_card:\n",
    "            card_in_play_onehot[card_in_play - self.min_card] = 1\n",
    "        M_transformed[2] = np.tile(card_in_play_onehot, (self.n_players, 1))\n",
    "\n",
    "        return M_transformed, b\n",
    "\n",
    "\n",
    "    def is_ended(self, state):\n",
    "        # print(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        if n_cards_in_deck == 0 and card_in_play == None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def compute_scores(self, state):\n",
    "        state = self.unpack_state(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for p_idx in range(self.n_players):\n",
    "            cards[p_idx].sort()\n",
    "\n",
    "            score = 0\n",
    "            if cards[p_idx]:\n",
    "                score += cards[p_idx][0]\n",
    "                last_card = cards[p_idx][0]\n",
    "\n",
    "                for card_idx in range(1, len(cards[p_idx])):\n",
    "                    new_card = cards[p_idx][card_idx]\n",
    "\n",
    "                    if not new_card == last_card + 1:\n",
    "                        score += new_card\n",
    "                    last_card = new_card\n",
    "\n",
    "            score -= coins[p_idx]\n",
    "\n",
    "            scores.append(score)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def winner(self, state):\n",
    "        \"\"\"Temporary winner: player with the lowest score even if the game is not ended.\"\"\"\n",
    "        state = self.unpack_state(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        if not self.is_ended(state):\n",
    "            return None\n",
    "        \n",
    "        scores = self.compute_scores(state)\n",
    "        min_score = 1000\n",
    "        lowest_scorers = []\n",
    "        # get lowest scorers (could be more than one)\n",
    "        for i, score in enumerate(scores):\n",
    "            if score < min_score:\n",
    "                lowest_scorers = [i]\n",
    "                min_score = score\n",
    "            if score <= min_score:\n",
    "                lowest_scorers.append(i)\n",
    "        \n",
    "        # if players are tied on lowest score, get the one with the fewest cards\n",
    "        if len(lowest_scorers) > 1:\n",
    "            min_n_cards = 1000\n",
    "            for i in lowest_scorers:\n",
    "                n_cards = len(cards[i])\n",
    "                if n_cards < min_n_cards:\n",
    "                    lowest_card_players = [i]\n",
    "                    min_n_cards = n_cards\n",
    "                elif n_cards <= min_n_cards:\n",
    "                    lowest_card_players.append(i)\n",
    "\n",
    "            if len(lowest_card_players) > 1:\n",
    "                winner = lowest_card_players[0]\n",
    "            else: # if still tied, pick a random winner (not the official rules)\n",
    "                winner = random.choice(lowest_card_players) \n",
    "        else:\n",
    "            winner = lowest_scorers[0]\n",
    "\n",
    "        return winner\n",
    "\n",
    "    \n",
    "    def basic_display_state(self, state):\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        print(\"Coins:           {0}\".format(coins))\n",
    "        print(\"Cards:           {0}\".format(cards))\n",
    "        print(\"Card in play:    {0}\".format(card_in_play))\n",
    "        print(\"Coins:           {0}\".format(coins_in_play))\n",
    "        print(\"Player:          {0}\".format(current_player))\n",
    "\n",
    "    def display_scores(self, state, players):\n",
    "        scores = self.compute_scores(state)\n",
    "        print(\"\")\n",
    "        print(\"--- Scores ---\")\n",
    "        for player in players:\n",
    "            print(\"{:<10} {:<10}\".format(\n",
    "                player.name, scores[player.turn])\n",
    "            )\n",
    "        print(\"\")\n",
    "\n",
    "    def display_state(self, state, players):\n",
    "        state = self.unpack_state(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n",
    "\n",
    "        scores = self.compute_scores(state)\n",
    "\n",
    "        def format_cards(card_list):\n",
    "            return \", \".join(map(str, sorted(card_list)))\n",
    "\n",
    "        player_labels = [player.name for player in players]\n",
    "        card_strings = [format_cards(cards[i]) for i in range(self.n_players)]\n",
    "        coin_strings = [str(coins[i]) for i in range(self.n_players)]\n",
    "        score_strings = [str(scores[i]) for i in range(self.n_players)]\n",
    "\n",
    "        max_card_len = max(20, max(len(card_str) for card_str in card_strings))\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n",
    "        print(\"\")\n",
    "        print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\"Player\", \"Cards\", max_card_len, \"Coins\", \"Score\"))\n",
    "        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n",
    "\n",
    "        for i in range(self.n_players):\n",
    "            print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\n",
    "                player_labels[i],\n",
    "                card_strings[i],\n",
    "                max_card_len,\n",
    "                coin_strings[i],\n",
    "                score_strings[i]\n",
    "            ))\n",
    "\n",
    "        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n",
    "        print(\"\\t\\t In play: [{0}]\".format(card_in_play))\n",
    "        print(\"\\t\\t Cards remaining: {0}\".format(n_cards_in_deck))\n",
    "        print(\"\\t\\t   Coins: {0}\".format(coins_in_play))\n",
    "        print(\"\")\n",
    "            \n",
    "    def pack_action(self, notation):\n",
    "        if notation == \"y\" or notation == \"Y\":\n",
    "            return ACTION_TAKE\n",
    "        else:\n",
    "            return ACTION_PASS\n",
    "\n",
    "    def current_player(self, state):\n",
    "        return state[2][3]\n",
    "    \n",
    "    def remaining_cards(self, state):\n",
    "        s = self.unpack(state)\n",
    "\n",
    "        # All cards in original deck\n",
    "        full_deck = set(self.initial_deck)  # This should be fixed when game starts\n",
    "\n",
    "        # Cards already taken by players\n",
    "        taken_cards = set()\n",
    "        for player_cards in s[\"player_cards\"]:\n",
    "            taken_cards.update(player_cards)\n",
    "\n",
    "        # Card currently in play\n",
    "        if s[\"card\"] is not None:\n",
    "            taken_cards.add(s[\"card\"])\n",
    "\n",
    "        # Cards already revealed (optional: e.g., discard pile if any)\n",
    "        # taken_cards.update(s.get(\"discarded_cards\", []))\n",
    "\n",
    "        # Remaining = full_deck - taken\n",
    "        remaining = full_deck - taken_cards\n",
    "        return list(remaining)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5554b136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.568800Z",
     "iopub.status.busy": "2025-04-16T03:27:08.568525Z",
     "iopub.status.idle": "2025-04-16T03:27:08.593520Z",
     "shell.execute_reply": "2025-04-16T03:27:08.592336Z"
    },
    "papermill": {
     "duration": 0.031211,
     "end_time": "2025-04-16T03:27:08.595527",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.564316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Players - UCT and PUCT Players [Open Loop MCTS]\n",
    "\n",
    "class Player(ABC):\n",
    "    \"\"\"The abstract class for a player. A player can be an AI agent (bot) or human.\"\"\"\n",
    "    def __init__(self, game, turn):\n",
    "        self.name = \"Player \" + str(turn)\n",
    "        self.game = game\n",
    "        self.turn = turn  # starting form 0 as convention in python\n",
    "        assert self.turn < self.game.n_players, \"Player turn out of range.\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BaseMCTSPlayer(Player, ABC):\n",
    "    def __init__(self, game, turn, thinking_time=1, simNum=0, max_moves=200):\n",
    "        super().__init__(game, turn)\n",
    "        self.thinking_time = thinking_time\n",
    "        self.simNum = simNum\n",
    "        self.max_moves = max_moves\n",
    "        self.max_depth = 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_action(self, state):\n",
    "        pass\n",
    "\n",
    "    def score(self, state, player, legal_actions, plays, wins):\n",
    "        total_ply = sum(plays[(\"decision\", player, state, a)] for a in legal_actions)\n",
    "        if total_ply == 0:\n",
    "            return 0\n",
    "        score = 0\n",
    "        for action in legal_actions:\n",
    "            key = (\"decision\", player, state, action)\n",
    "            if plays[key]:\n",
    "                score += (wins[key] / plays[key]) * (plays[key] / total_ply)\n",
    "        return score\n",
    "\n",
    "\n",
    "class UCTPlayer(BaseMCTSPlayer):\n",
    "    def __init__(self, game, turn=0, thinking_time=1, simNum=0):\n",
    "        super().__init__(game, turn, thinking_time, simNum)\n",
    "        self.C = 1.4  # Exploration parameter\n",
    "\n",
    "    def get_action(self, state):\n",
    "        board = self.game\n",
    "        player = board.current_player(state)\n",
    "        legal_actions = board.legal_actions(state)\n",
    "\n",
    "        if not legal_actions:\n",
    "            return None, None\n",
    "        if len(legal_actions) == 1:\n",
    "            return legal_actions[0], 0\n",
    "\n",
    "        plays = defaultdict(int)\n",
    "        wins = defaultdict(int)\n",
    "        games = 0\n",
    "\n",
    "        if self.thinking_time > 0 and self.simNum == 0:\n",
    "            start_time = time.perf_counter()\n",
    "            while time.perf_counter() - start_time < self.thinking_time:\n",
    "                self.run_simulation(state, board, plays, wins)\n",
    "                games += 1\n",
    "        else:\n",
    "            for _ in range(self.simNum):\n",
    "                self.run_simulation(state, board, plays, wins)\n",
    "                games += 1\n",
    "\n",
    "        random.shuffle(legal_actions)\n",
    "        action = max(\n",
    "            legal_actions,\n",
    "            key=lambda a: plays.get((player, state, a), 0)\n",
    "        )\n",
    "\n",
    "        return action, self.score(state, player, legal_actions, plays, wins)\n",
    "\n",
    "    def run_simulation(self, state, board, plays, wins):\n",
    "        \"\"\"Run a single MCTS simulation.\"\"\"\n",
    "        tree = set()\n",
    "        player = board.current_player(state)\n",
    "\n",
    "        # === Selection & Expansion ===\n",
    "        for t in range(1, self.max_moves + 1):\n",
    "            legal_actions = board.legal_actions(state)\n",
    "\n",
    "            # Selection using UCB1 if data exists for all actions\n",
    "            if all(plays.get((player, state, a)) for a in legal_actions):\n",
    "                log_total = log(sum(plays[(player, state, a)] for a in legal_actions))\n",
    "                action = max(\n",
    "                    legal_actions,\n",
    "                    key=lambda a: (\n",
    "                        wins[(player, state, a)] / plays[(player, state, a)] +\n",
    "                        self.C * sqrt(log_total / plays[(player, state, a)])\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                # Expansion – If any action is unexplored, take a random one\n",
    "                action = random.choice(legal_actions)\n",
    "                if (player, state, action) not in plays:\n",
    "                    plays[(player, state, action)] = 0\n",
    "                    wins[(player, state, action)] = 0\n",
    "                    if t > self.max_depth:\n",
    "                        self.max_depth = t\n",
    "\n",
    "            tree.add((player, state, action))\n",
    "            state = board.next_state(state, action)\n",
    "            player = board.current_player(state)\n",
    "\n",
    "            # Check for game-ending state\n",
    "            winner = board.winner(state)\n",
    "            if winner is not None:\n",
    "                break\n",
    "\n",
    "        # === Backpropagation ===\n",
    "        for player, state, action in tree:\n",
    "            plays[(player, state, action)] += 1\n",
    "            if player == winner:\n",
    "                wins[(player, state, action)] += 1\n",
    "\n",
    "\n",
    "class PUCTPlayer(BaseMCTSPlayer):\n",
    "    def __init__(self, game, turn=0, thinking_time=1, simNum=0):\n",
    "        super().__init__(game, turn, thinking_time, simNum)\n",
    "        self.C = 1.5 # c_puct exploration parameter\n",
    "        self.prior = lambda state, action: 1 / len(self.game.legal_actions(state))\n",
    "        self.value = None\n",
    "\n",
    "    def get_action(self, state):\n",
    "        board = self.game\n",
    "        player = board.current_player(state)\n",
    "        legal_actions = board.legal_actions(state)\n",
    "\n",
    "        if not legal_actions:\n",
    "            return None, None\n",
    "        if len(legal_actions) == 1:\n",
    "            return legal_actions[0], 0\n",
    "\n",
    "        plays = defaultdict(int)\n",
    "        wins = defaultdict(int)\n",
    "        games = 0\n",
    "\n",
    "        if self.thinking_time > 0 and self.simNum == 0:\n",
    "            start_time = time.perf_counter()\n",
    "            while time.perf_counter() - start_time < self.thinking_time:\n",
    "                self.run_simulation(state, board, plays, wins)\n",
    "                games += 1\n",
    "        else:\n",
    "            for _ in range(self.simNum):\n",
    "                self.run_simulation(state, board, plays, wins)\n",
    "                games += 1\n",
    "\n",
    "        random.shuffle(legal_actions)\n",
    "        action = max(\n",
    "            legal_actions,\n",
    "            key=lambda a: plays.get((player, state, a), 0)\n",
    "        )\n",
    "\n",
    "        return action, self.score(state, player, legal_actions, plays, wins)\n",
    "\n",
    "    def run_simulation(self, state, board, plays, wins):\n",
    "        \"\"\"Run a single MCTS simulation.\"\"\"\n",
    "        tree = set()\n",
    "        player = board.current_player(state)\n",
    "\n",
    "        # === Selection & Expansion ===\n",
    "        for t in range(1, self.max_moves + 1):\n",
    "            legal_actions = board.legal_actions(state)\n",
    "\n",
    "            # Selection using UCB1 if data exists for all actions\n",
    "            if all(plays.get((player, state, a)) for a in legal_actions):\n",
    "                total = sum(plays[(player, state, a)] for a in legal_actions)\n",
    "                # for a in legal_actions:\n",
    "                #     print(\"Check Total:\", a, wins[(player, state, a)] / plays[(player, state, a)])\n",
    "                action = max(\n",
    "                    legal_actions,\n",
    "                    key=lambda a: (\n",
    "                        wins[(player, state, a)] / plays[(player, state, a)] +\n",
    "                        self.C * self.prior(state, a) * sqrt(total / plays[(player, state, a)])\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                # Expansion – If any action is unexplored, take a random one\n",
    "                action = random.choice(legal_actions)\n",
    "                if (player, state, action) not in plays:\n",
    "                    plays[(player, state, action)] = 0\n",
    "                    wins[(player, state, action)] = 0\n",
    "                    if t > self.max_depth:\n",
    "                        self.max_depth = t\n",
    "\n",
    "            tree.add((player, state, action)) # trajectory\n",
    "            state = board.next_state(state, action)\n",
    "            player = board.current_player(state)\n",
    "\n",
    "            # Check for game-ending state\n",
    "            winner = board.winner(state)\n",
    "            if winner is not None:\n",
    "                break\n",
    "\n",
    "        # === Backpropagation ===\n",
    "        for player, state, action in tree:\n",
    "            plays[(player, state, action)] += 1\n",
    "            if player == winner:\n",
    "                wins[(player, state, action)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eef5c79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.604654Z",
     "iopub.status.busy": "2025-04-16T03:27:08.604112Z",
     "iopub.status.idle": "2025-04-16T03:27:08.614347Z",
     "shell.execute_reply": "2025-04-16T03:27:08.613514Z"
    },
    "papermill": {
     "duration": 0.01628,
     "end_time": "2025-04-16T03:27:08.615686",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.599406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Utils - prior functions\n",
    "\n",
    "def nn_prior_fn(model, game):\n",
    "    def nn_prior(state, action):\n",
    "        with torch.no_grad():\n",
    "            M, b = game.standard_state(state)\n",
    "            M_tensor = torch.tensor(M, dtype=torch.float32).unsqueeze(0)\n",
    "            b_tensor = torch.tensor(b, dtype=torch.float32).unsqueeze(0)\n",
    "            policy = model(M_tensor, b_tensor)[0]\n",
    "            prob = policy.item()\n",
    "            return (prob if action == ACTION_PASS else 1 - prob)\n",
    "    return nn_prior\n",
    "\n",
    "def smart_prior_fn(game, p=0.98):\n",
    "    def smart_prior(state, action):\n",
    "        state = game.unpack_state(state)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state \n",
    "        other_cards = [i for i in list(chain.from_iterable(cards)) if i not in cards[current_player]]\n",
    "        good_for_me = any(abs(card_in_play - card) < 2 for card in cards[current_player])\n",
    "        good_for_them = any(abs(card_in_play - card) < 2 for card in other_cards)\n",
    "        least_chip = min(coins)\n",
    "        legal_actions = game.legal_actions(state)\n",
    "\n",
    "        if good_for_me:\n",
    "            if good_for_them:\n",
    "                good_action = ACTION_TAKE\n",
    "            else:\n",
    "                good_action = ACTION_TAKE\n",
    "                if least_chip > 2:\n",
    "                    good_action = ACTION_PASS\n",
    "        else:\n",
    "            good_action = ACTION_PASS\n",
    "            if coins[current_player] < 2 or abs(coins_in_play - card_in_play) < min(3, card_in_play//2):\n",
    "                good_action = ACTION_TAKE\n",
    "\n",
    "        if action not in legal_actions:\n",
    "            return 0  # Invalid action for the state\n",
    "        return p if action == good_action else (1 - p)\n",
    "    \n",
    "    return smart_prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3c25ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.625649Z",
     "iopub.status.busy": "2025-04-16T03:27:08.625360Z",
     "iopub.status.idle": "2025-04-16T03:27:08.634654Z",
     "shell.execute_reply": "2025-04-16T03:27:08.633736Z"
    },
    "papermill": {
     "duration": 0.017111,
     "end_time": "2025-04-16T03:27:08.636111",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.619000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Utils - play and performance evaluation\n",
    "\n",
    "def play(game, players, display=True):    \n",
    "    players.sort(key=lambda x: x.turn)\n",
    "    current_player = players[0].turn\n",
    "    \n",
    "    state = game.starting_state(current_player=current_player)\n",
    "    state = game.pack_state(state)\n",
    "\n",
    "    while not game.is_ended(state):\n",
    "        player = players[current_player]\n",
    "        action, score = player.get_action(state)\n",
    "        state = game.next_state(state, action)\n",
    "        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = game.unpack_state(state)\n",
    "        if display:\n",
    "            game.display_state(state, players)\n",
    "        \n",
    "        # print(game.standard_state(state))  \n",
    "    winner = game.winner(state)\n",
    "    if display:\n",
    "        game.display_scores(state, players)\n",
    "        print(\"Game ended. Player\", winner, \"wins!\")\n",
    "\n",
    "    return winner\n",
    "\n",
    "def eval_performance(game, target_player, opponents, num_games=100, verbose=False):\n",
    "    random.seed(time.time())\n",
    "    target_player.name = \"Target\"\n",
    "    players = [target_player] + opponents\n",
    "    win = defaultdict(int)\n",
    "    for i in bar(range(num_games)):\n",
    "        target_player.turn = i % len(players)\n",
    "        for j, player in enumerate(players):\n",
    "            if player != target_player:\n",
    "                player.turn = (i + j) % len(players)\n",
    "            # print(f\"Game {i+1}: Player {player.name} turn: {player.turn}\")\n",
    "        winner = play(game, players, display=False)\n",
    "        win[players[winner].name] += 1\n",
    "        if verbose and i in [150, 180, 210, 240, 270]:\n",
    "            print(f\"Number of wins for each player: {win}\")\n",
    "\n",
    "    print(f\"Number of wins for each player: {win}\")\n",
    "    winrate = win[target_player.name] / num_games\n",
    "\n",
    "    return winrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc7c11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.648630Z",
     "iopub.status.busy": "2025-04-16T03:27:08.648297Z",
     "iopub.status.idle": "2025-04-16T03:27:08.663616Z",
     "shell.execute_reply": "2025-04-16T03:27:08.662886Z"
    },
    "papermill": {
     "duration": 0.022371,
     "end_time": "2025-04-16T03:27:08.664952",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.642581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Training process functions\n",
    "\n",
    "def self_play(game, players, times=1, to_file=None, smart=False):\n",
    "    if smart:\n",
    "        for player in players:\n",
    "            player.prior = smart_prior_fn(game)\n",
    "\n",
    "    data = {\"state\": [], \"policy\": [], \"value\": []}\n",
    "    for _ in bar(range(times)):\n",
    "        state = game.starting_state(current_player=0)\n",
    "        state = game.pack_state(state)\n",
    "        current_player = 0\n",
    "\n",
    "        while not game.is_ended(state):\n",
    "            player = players[current_player]\n",
    "            action, score = player.get_action(state)\n",
    "            state = game.next_state(state, action)\n",
    "            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = game.unpack_state(state)\n",
    "            if card_in_play is not None:\n",
    "                data[\"state\"].append(game.standard_state(state))  # Append (M, b) as NumPy arrays\n",
    "                data[\"policy\"].append(action)\n",
    "                data[\"value\"].append(score)\n",
    "\n",
    "    # Convert NumPy arrays in \"state\" to lists for JSON serialization\n",
    "    if to_file is not None:\n",
    "        serializable_data = {\n",
    "            \"state\": [(M.tolist(), b.tolist()) for M, b in data[\"state\"]],\n",
    "            \"policy\": data[\"policy\"],\n",
    "            \"value\": data[\"value\"]\n",
    "        }\n",
    "        with open(to_file, \"w\") as f:\n",
    "            json.dump(serializable_data, f)\n",
    "\n",
    "    return data\n",
    "\n",
    "def rl_train(rounds=10, num_games=4, simNum=300, prior=False, ctd_from=0):\n",
    "    game = NoThanksBoard(n_players=3)\n",
    "    Player_0 = PUCTPlayer(game=game, turn=0, simNum=simNum)\n",
    "    Player_1 = PUCTPlayer(game=game, turn=1, simNum=simNum)\n",
    "    Player_2 = PUCTPlayer(game=game, turn=2, simNum=simNum)\n",
    "    players = [Player_0, Player_1, Player_2]\n",
    "\n",
    "    tester = PUCTPlayer(game=game, turn=0, simNum=simNum)\n",
    "\n",
    "    batch_size = 512\n",
    "    n_players = 3\n",
    "    model = PolicyValueNet(n_players, hidden_dim=32)\n",
    "\n",
    "    if prior == True:\n",
    "        model.load_state_dict(torch.load(f'policy_value_net_rd{ctd_from}.pth', weights_only=True))\n",
    "        model.eval()\n",
    "        for player in players:\n",
    "            player.prior = nn_prior_fn(model, game)\n",
    "            player.simNum = simNum\n",
    "        print(f\"Model {ctd_from} loaded...Priors are updated for players.\")\n",
    "\n",
    "    for i in range(rounds):\n",
    "        print(f\"Round {i}: The bots are playing...\")\n",
    "\n",
    "        # smart = True if i == 0 else False\n",
    "        smart = False\n",
    "        data = self_play(game, players, times=num_games, smart=smart)\n",
    "        \n",
    "        # Combine and shuffle the data\n",
    "        combined_data = list(zip(data[\"state\"], data[\"policy\"], data[\"value\"]))\n",
    "        random.shuffle(combined_data)\n",
    "        data[\"state\"], data[\"policy\"], data[\"value\"] = zip(*combined_data)\n",
    "\n",
    "        states = data[\"state\"]\n",
    "        target_policy = np.array(data[\"policy\"])\n",
    "        target_value = np.array(data[\"value\"])\n",
    "        print(\"Training Data prepared.\")\n",
    "        num_samples = len(states)\n",
    "        num_batches = num_samples // batch_size\n",
    "        print(f\"Total samples: {num_samples}, Batches: {num_batches}\")\n",
    "\n",
    "        backup_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        for batch_idx in range(num_batches):\n",
    "            # Extract batch data\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            \n",
    "            batch_states = states[batch_start:batch_end]\n",
    "            batch_policies = target_policy[batch_start:batch_end]\n",
    "            batch_values = target_value[batch_start:batch_end]\n",
    "            \n",
    "            # Reshape states into (M, b) format\n",
    "            M = np.array([s[0] for s in batch_states])\n",
    "            b = np.array([np.array(s[1], dtype=np.float32) for s in batch_states], dtype=np.float32)\n",
    "\n",
    "            # Train the model on the batch\n",
    "            model.train()\n",
    "            print(f\"Training batch {batch_idx + 1}/{num_batches}\")\n",
    "            train_nn(model, batch_size, n_players, 32, (M, b), batch_policies, batch_values)\n",
    "\n",
    "        # Save the model\n",
    "        torch.save(model.state_dict(), f'policy_value_net_rd{i+ctd_from}.pth')\n",
    "\n",
    "        print(f\"Round {i} completed. Evaluating performance...\")\n",
    "        \n",
    "        # Update prior for the tester\n",
    "        model.eval()\n",
    "        tester.prior = nn_prior_fn(model, game)\n",
    "\n",
    "        # Evaluate the performance of the trained model\n",
    "        # need about 300 games to reach stable estimate of winrate\n",
    "        winrate = eval_performance(game, tester, players[1:], num_games=150, verbose=False)\n",
    "        if winrate > 0.4:\n",
    "            print(f\"Winrate of the Target Player: {winrate:.2%}; Model is accepted.\")\n",
    "            for player in players:\n",
    "                player.prior = nn_prior_fn(model, game)\n",
    "        else:\n",
    "            print(f\"Winrate of the Target Player: {winrate:.2%}; Model is rejected.\")\n",
    "            model.load_state_dict(backup_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "276d9b4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T03:27:08.672502Z",
     "iopub.status.busy": "2025-04-16T03:27:08.672201Z",
     "iopub.status.idle": "2025-04-16T08:51:56.309548Z",
     "shell.execute_reply": "2025-04-16T08:51:56.308718Z"
    },
    "papermill": {
     "duration": 19487.80228,
     "end_time": "2025-04-16T08:51:56.470606",
     "exception": false,
     "start_time": "2025-04-16T03:27:08.668326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0: The bots are playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [13:46<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data prepared.\n",
      "Total samples: 20960, Batches: 40\n",
      "Training batch 1/40\n",
      "Loss = 0.7456, Policy Loss = 0.6948, Value Loss = 0.2132\n",
      "Training batch 2/40\n",
      "Loss = 0.7048, Policy Loss = 0.6541, Value Loss = 0.2105\n",
      "Training batch 3/40\n",
      "Loss = 0.6879, Policy Loss = 0.6372, Value Loss = 0.2077\n",
      "Training batch 4/40\n",
      "Loss = 0.6603, Policy Loss = 0.6098, Value Loss = 0.2031\n",
      "Training batch 5/40\n",
      "Loss = 0.6314, Policy Loss = 0.5809, Value Loss = 0.2019\n",
      "Training batch 6/40\n",
      "Loss = 0.6091, Policy Loss = 0.5588, Value Loss = 0.1968\n",
      "Training batch 7/40\n",
      "Loss = 0.6067, Policy Loss = 0.5564, Value Loss = 0.1937\n",
      "Training batch 8/40\n",
      "Loss = 0.5684, Policy Loss = 0.5182, Value Loss = 0.1904\n",
      "Training batch 9/40\n",
      "Loss = 0.5607, Policy Loss = 0.5106, Value Loss = 0.1855\n",
      "Training batch 10/40\n",
      "Loss = 0.5134, Policy Loss = 0.4634, Value Loss = 0.1823\n",
      "Training batch 11/40\n",
      "Loss = 0.5095, Policy Loss = 0.4596, Value Loss = 0.1808\n",
      "Training batch 12/40\n",
      "Loss = 0.4901, Policy Loss = 0.4402, Value Loss = 0.1759\n",
      "Training batch 13/40\n",
      "Loss = 0.4899, Policy Loss = 0.4401, Value Loss = 0.1736\n",
      "Training batch 14/40\n",
      "Loss = 0.4612, Policy Loss = 0.4115, Value Loss = 0.1688\n",
      "Training batch 15/40\n",
      "Loss = 0.4494, Policy Loss = 0.3997, Value Loss = 0.1663\n",
      "Training batch 16/40\n",
      "Loss = 0.4315, Policy Loss = 0.3820, Value Loss = 0.1635\n",
      "Training batch 17/40\n",
      "Loss = 0.4207, Policy Loss = 0.3712, Value Loss = 0.1595\n",
      "Training batch 18/40\n",
      "Loss = 0.3896, Policy Loss = 0.3402, Value Loss = 0.1559\n",
      "Training batch 19/40\n",
      "Loss = 0.3926, Policy Loss = 0.3433, Value Loss = 0.1524\n",
      "Training batch 20/40\n",
      "Loss = 0.3824, Policy Loss = 0.3332, Value Loss = 0.1497\n",
      "Training batch 21/40\n",
      "Loss = 0.3626, Policy Loss = 0.3135, Value Loss = 0.1462\n",
      "Training batch 22/40\n",
      "Loss = 0.3652, Policy Loss = 0.3161, Value Loss = 0.1432\n",
      "Training batch 23/40\n",
      "Loss = 0.3723, Policy Loss = 0.3233, Value Loss = 0.1402\n",
      "Training batch 24/40\n",
      "Loss = 0.3550, Policy Loss = 0.3061, Value Loss = 0.1371\n",
      "Training batch 25/40\n",
      "Loss = 0.3293, Policy Loss = 0.2805, Value Loss = 0.1340\n",
      "Training batch 26/40\n",
      "Loss = 0.3372, Policy Loss = 0.2885, Value Loss = 0.1292\n",
      "Training batch 27/40\n",
      "Loss = 0.3486, Policy Loss = 0.2999, Value Loss = 0.1274\n",
      "Training batch 28/40\n",
      "Loss = 0.3220, Policy Loss = 0.2735, Value Loss = 0.1235\n",
      "Training batch 29/40\n",
      "Loss = 0.3218, Policy Loss = 0.2734, Value Loss = 0.1205\n",
      "Training batch 30/40\n",
      "Loss = 0.3050, Policy Loss = 0.2567, Value Loss = 0.1174\n",
      "Training batch 31/40\n",
      "Loss = 0.3046, Policy Loss = 0.2563, Value Loss = 0.1146\n",
      "Training batch 32/40\n",
      "Loss = 0.2962, Policy Loss = 0.2480, Value Loss = 0.1124\n",
      "Training batch 33/40\n",
      "Loss = 0.3066, Policy Loss = 0.2584, Value Loss = 0.1095\n",
      "Training batch 34/40\n",
      "Loss = 0.2879, Policy Loss = 0.2399, Value Loss = 0.1064\n",
      "Training batch 35/40\n",
      "Loss = 0.2813, Policy Loss = 0.2334, Value Loss = 0.1044\n",
      "Training batch 36/40\n",
      "Loss = 0.2792, Policy Loss = 0.2314, Value Loss = 0.1010\n",
      "Training batch 37/40\n",
      "Loss = 0.2872, Policy Loss = 0.2395, Value Loss = 0.0987\n",
      "Training batch 38/40\n",
      "Loss = 0.2755, Policy Loss = 0.2279, Value Loss = 0.0960\n",
      "Training batch 39/40\n",
      "Loss = 0.2693, Policy Loss = 0.2217, Value Loss = 0.0939\n",
      "Training batch 40/40\n",
      "Loss = 0.2622, Policy Loss = 0.2148, Value Loss = 0.0911\n",
      "Round 0 completed. Evaluating performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [49:49<00:00, 19.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins for each player: defaultdict(<class 'int'>, {'Player 1': 65, 'Player 2': 84, 'Target': 1})\n",
      "Winrate of the Target Player: 0.67%; Model is rejected.\n",
      "Round 1: The bots are playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [14:16<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data prepared.\n",
      "Total samples: 20808, Batches: 40\n",
      "Training batch 1/40\n",
      "Loss = 0.7689, Policy Loss = 0.7181, Value Loss = 0.2138\n",
      "Training batch 2/40\n",
      "Loss = 0.7282, Policy Loss = 0.6775, Value Loss = 0.2095\n",
      "Training batch 3/40\n",
      "Loss = 0.6957, Policy Loss = 0.6451, Value Loss = 0.2066\n",
      "Training batch 4/40\n",
      "Loss = 0.6396, Policy Loss = 0.5890, Value Loss = 0.2020\n",
      "Training batch 5/40\n",
      "Loss = 0.6367, Policy Loss = 0.5863, Value Loss = 0.1999\n",
      "Training batch 6/40\n",
      "Loss = 0.6256, Policy Loss = 0.5752, Value Loss = 0.1948\n",
      "Training batch 7/40\n",
      "Loss = 0.5910, Policy Loss = 0.5407, Value Loss = 0.1916\n",
      "Training batch 8/40\n",
      "Loss = 0.5536, Policy Loss = 0.5034, Value Loss = 0.1894\n",
      "Training batch 9/40\n",
      "Loss = 0.5333, Policy Loss = 0.4832, Value Loss = 0.1849\n",
      "Training batch 10/40\n",
      "Loss = 0.5650, Policy Loss = 0.5150, Value Loss = 0.1820\n",
      "Training batch 11/40\n",
      "Loss = 0.4994, Policy Loss = 0.4494, Value Loss = 0.1787\n",
      "Training batch 12/40\n",
      "Loss = 0.4796, Policy Loss = 0.4297, Value Loss = 0.1764\n",
      "Training batch 13/40\n",
      "Loss = 0.4815, Policy Loss = 0.4317, Value Loss = 0.1710\n",
      "Training batch 14/40\n",
      "Loss = 0.4824, Policy Loss = 0.4327, Value Loss = 0.1674\n",
      "Training batch 15/40\n",
      "Loss = 0.4396, Policy Loss = 0.3900, Value Loss = 0.1642\n",
      "Training batch 16/40\n",
      "Loss = 0.4162, Policy Loss = 0.3667, Value Loss = 0.1605\n",
      "Training batch 17/40\n",
      "Loss = 0.4105, Policy Loss = 0.3610, Value Loss = 0.1583\n",
      "Training batch 18/40\n",
      "Loss = 0.4311, Policy Loss = 0.3817, Value Loss = 0.1545\n",
      "Training batch 19/40\n",
      "Loss = 0.3963, Policy Loss = 0.3470, Value Loss = 0.1518\n",
      "Training batch 20/40\n",
      "Loss = 0.3698, Policy Loss = 0.3207, Value Loss = 0.1480\n",
      "Training batch 21/40\n",
      "Loss = 0.3643, Policy Loss = 0.3152, Value Loss = 0.1442\n",
      "Training batch 22/40\n",
      "Loss = 0.3734, Policy Loss = 0.3244, Value Loss = 0.1411\n",
      "Training batch 23/40\n",
      "Loss = 0.3689, Policy Loss = 0.3199, Value Loss = 0.1377\n",
      "Training batch 24/40\n",
      "Loss = 0.3472, Policy Loss = 0.2984, Value Loss = 0.1354\n",
      "Training batch 25/40\n",
      "Loss = 0.3458, Policy Loss = 0.2971, Value Loss = 0.1327\n",
      "Training batch 26/40\n",
      "Loss = 0.3535, Policy Loss = 0.3048, Value Loss = 0.1291\n",
      "Training batch 27/40\n",
      "Loss = 0.3398, Policy Loss = 0.2912, Value Loss = 0.1259\n",
      "Training batch 28/40\n",
      "Loss = 0.3210, Policy Loss = 0.2725, Value Loss = 0.1230\n",
      "Training batch 29/40\n",
      "Loss = 0.3132, Policy Loss = 0.2648, Value Loss = 0.1205\n",
      "Training batch 30/40\n",
      "Loss = 0.3048, Policy Loss = 0.2564, Value Loss = 0.1178\n",
      "Training batch 31/40\n",
      "Loss = 0.3226, Policy Loss = 0.2743, Value Loss = 0.1151\n",
      "Training batch 32/40\n",
      "Loss = 0.2996, Policy Loss = 0.2514, Value Loss = 0.1131\n",
      "Training batch 33/40\n",
      "Loss = 0.2850, Policy Loss = 0.2369, Value Loss = 0.1093\n",
      "Training batch 34/40\n",
      "Loss = 0.2801, Policy Loss = 0.2321, Value Loss = 0.1091\n",
      "Training batch 35/40\n",
      "Loss = 0.2781, Policy Loss = 0.2301, Value Loss = 0.1053\n",
      "Training batch 36/40\n",
      "Loss = 0.2834, Policy Loss = 0.2356, Value Loss = 0.1024\n",
      "Training batch 37/40\n",
      "Loss = 0.2651, Policy Loss = 0.2174, Value Loss = 0.0998\n",
      "Training batch 38/40\n",
      "Loss = 0.2786, Policy Loss = 0.2309, Value Loss = 0.0973\n",
      "Training batch 39/40\n",
      "Loss = 0.2591, Policy Loss = 0.2115, Value Loss = 0.0946\n",
      "Training batch 40/40\n",
      "Loss = 0.2673, Policy Loss = 0.2198, Value Loss = 0.0922\n",
      "Round 1 completed. Evaluating performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [49:44<00:00, 19.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins for each player: defaultdict(<class 'int'>, {'Player 1': 66, 'Player 2': 83, 'Target': 1})\n",
      "Winrate of the Target Player: 0.67%; Model is rejected.\n",
      "Round 2: The bots are playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [14:19<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data prepared.\n",
      "Total samples: 20725, Batches: 40\n",
      "Training batch 1/40\n",
      "Loss = 0.7536, Policy Loss = 0.7028, Value Loss = 0.2141\n",
      "Training batch 2/40\n",
      "Loss = 0.7363, Policy Loss = 0.6856, Value Loss = 0.2089\n",
      "Training batch 3/40\n",
      "Loss = 0.6935, Policy Loss = 0.6429, Value Loss = 0.2057\n",
      "Training batch 4/40\n",
      "Loss = 0.6565, Policy Loss = 0.6060, Value Loss = 0.2015\n",
      "Training batch 5/40\n",
      "Loss = 0.6249, Policy Loss = 0.5745, Value Loss = 0.1977\n",
      "Training batch 6/40\n",
      "Loss = 0.6104, Policy Loss = 0.5601, Value Loss = 0.1949\n",
      "Training batch 7/40\n",
      "Loss = 0.6048, Policy Loss = 0.5545, Value Loss = 0.1916\n",
      "Training batch 8/40\n",
      "Loss = 0.5719, Policy Loss = 0.5217, Value Loss = 0.1880\n",
      "Training batch 9/40\n",
      "Loss = 0.5447, Policy Loss = 0.4946, Value Loss = 0.1857\n",
      "Training batch 10/40\n",
      "Loss = 0.5160, Policy Loss = 0.4660, Value Loss = 0.1812\n",
      "Training batch 11/40\n",
      "Loss = 0.5076, Policy Loss = 0.4577, Value Loss = 0.1775\n",
      "Training batch 12/40\n",
      "Loss = 0.4936, Policy Loss = 0.4437, Value Loss = 0.1720\n",
      "Training batch 13/40\n",
      "Loss = 0.4750, Policy Loss = 0.4253, Value Loss = 0.1705\n",
      "Training batch 14/40\n",
      "Loss = 0.4360, Policy Loss = 0.3863, Value Loss = 0.1667\n",
      "Training batch 15/40\n",
      "Loss = 0.4298, Policy Loss = 0.3803, Value Loss = 0.1636\n",
      "Training batch 16/40\n",
      "Loss = 0.4314, Policy Loss = 0.3819, Value Loss = 0.1599\n",
      "Training batch 17/40\n",
      "Loss = 0.3974, Policy Loss = 0.3480, Value Loss = 0.1576\n",
      "Training batch 18/40\n",
      "Loss = 0.4065, Policy Loss = 0.3571, Value Loss = 0.1529\n",
      "Training batch 19/40\n",
      "Loss = 0.3801, Policy Loss = 0.3308, Value Loss = 0.1495\n",
      "Training batch 20/40\n",
      "Loss = 0.3913, Policy Loss = 0.3421, Value Loss = 0.1475\n",
      "Training batch 21/40\n",
      "Loss = 0.3670, Policy Loss = 0.3179, Value Loss = 0.1441\n",
      "Training batch 22/40\n",
      "Loss = 0.3692, Policy Loss = 0.3202, Value Loss = 0.1403\n",
      "Training batch 23/40\n",
      "Loss = 0.3665, Policy Loss = 0.3176, Value Loss = 0.1365\n",
      "Training batch 24/40\n",
      "Loss = 0.3617, Policy Loss = 0.3129, Value Loss = 0.1343\n",
      "Training batch 25/40\n",
      "Loss = 0.3460, Policy Loss = 0.2973, Value Loss = 0.1315\n",
      "Training batch 26/40\n",
      "Loss = 0.3381, Policy Loss = 0.2894, Value Loss = 0.1276\n",
      "Training batch 27/40\n",
      "Loss = 0.3528, Policy Loss = 0.3042, Value Loss = 0.1239\n",
      "Training batch 28/40\n",
      "Loss = 0.3082, Policy Loss = 0.2597, Value Loss = 0.1225\n",
      "Training batch 29/40\n",
      "Loss = 0.3144, Policy Loss = 0.2660, Value Loss = 0.1183\n",
      "Training batch 30/40\n",
      "Loss = 0.3014, Policy Loss = 0.2531, Value Loss = 0.1160\n",
      "Training batch 31/40\n",
      "Loss = 0.3152, Policy Loss = 0.2670, Value Loss = 0.1120\n",
      "Training batch 32/40\n",
      "Loss = 0.2982, Policy Loss = 0.2501, Value Loss = 0.1099\n",
      "Training batch 33/40\n",
      "Loss = 0.2894, Policy Loss = 0.2413, Value Loss = 0.1073\n",
      "Training batch 34/40\n",
      "Loss = 0.2794, Policy Loss = 0.2314, Value Loss = 0.1045\n",
      "Training batch 35/40\n",
      "Loss = 0.2922, Policy Loss = 0.2444, Value Loss = 0.1015\n",
      "Training batch 36/40\n",
      "Loss = 0.2768, Policy Loss = 0.2290, Value Loss = 0.0995\n",
      "Training batch 37/40\n",
      "Loss = 0.2790, Policy Loss = 0.2313, Value Loss = 0.0964\n",
      "Training batch 38/40\n",
      "Loss = 0.2687, Policy Loss = 0.2211, Value Loss = 0.0942\n",
      "Training batch 39/40\n",
      "Loss = 0.2596, Policy Loss = 0.2121, Value Loss = 0.0924\n",
      "Training batch 40/40\n",
      "Loss = 0.2685, Policy Loss = 0.2211, Value Loss = 0.0892\n",
      "Round 2 completed. Evaluating performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [50:07<00:00, 20.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins for each player: defaultdict(<class 'int'>, {'Player 1': 68, 'Player 2': 82})\n",
      "Winrate of the Target Player: 0.00%; Model is rejected.\n",
      "Round 3: The bots are playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [14:41<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data prepared.\n",
      "Total samples: 21268, Batches: 41\n",
      "Training batch 1/41\n",
      "Loss = 0.7637, Policy Loss = 0.7129, Value Loss = 0.2131\n",
      "Training batch 2/41\n",
      "Loss = 0.7096, Policy Loss = 0.6589, Value Loss = 0.2092\n",
      "Training batch 3/41\n",
      "Loss = 0.6869, Policy Loss = 0.6363, Value Loss = 0.2072\n",
      "Training batch 4/41\n",
      "Loss = 0.6550, Policy Loss = 0.6045, Value Loss = 0.2020\n",
      "Training batch 5/41\n",
      "Loss = 0.6485, Policy Loss = 0.5980, Value Loss = 0.1988\n",
      "Training batch 6/41\n",
      "Loss = 0.6114, Policy Loss = 0.5611, Value Loss = 0.1954\n",
      "Training batch 7/41\n",
      "Loss = 0.5996, Policy Loss = 0.5493, Value Loss = 0.1930\n",
      "Training batch 8/41\n",
      "Loss = 0.5798, Policy Loss = 0.5296, Value Loss = 0.1886\n",
      "Training batch 9/41\n",
      "Loss = 0.5299, Policy Loss = 0.4798, Value Loss = 0.1853\n",
      "Training batch 10/41\n",
      "Loss = 0.5207, Policy Loss = 0.4707, Value Loss = 0.1813\n",
      "Training batch 11/41\n",
      "Loss = 0.4980, Policy Loss = 0.4480, Value Loss = 0.1792\n",
      "Training batch 12/41\n",
      "Loss = 0.4833, Policy Loss = 0.4335, Value Loss = 0.1752\n",
      "Training batch 13/41\n",
      "Loss = 0.4768, Policy Loss = 0.4270, Value Loss = 0.1713\n",
      "Training batch 14/41\n",
      "Loss = 0.4538, Policy Loss = 0.4041, Value Loss = 0.1687\n",
      "Training batch 15/41\n",
      "Loss = 0.4270, Policy Loss = 0.3774, Value Loss = 0.1643\n",
      "Training batch 16/41\n",
      "Loss = 0.4267, Policy Loss = 0.3772, Value Loss = 0.1613\n",
      "Training batch 17/41\n",
      "Loss = 0.4226, Policy Loss = 0.3732, Value Loss = 0.1583\n",
      "Training batch 18/41\n",
      "Loss = 0.4172, Policy Loss = 0.3678, Value Loss = 0.1555\n",
      "Training batch 19/41\n",
      "Loss = 0.3809, Policy Loss = 0.3316, Value Loss = 0.1504\n",
      "Training batch 20/41\n",
      "Loss = 0.3876, Policy Loss = 0.3384, Value Loss = 0.1477\n",
      "Training batch 21/41\n",
      "Loss = 0.3607, Policy Loss = 0.3116, Value Loss = 0.1456\n",
      "Training batch 22/41\n",
      "Loss = 0.3483, Policy Loss = 0.2992, Value Loss = 0.1412\n",
      "Training batch 23/41\n",
      "Loss = 0.3478, Policy Loss = 0.2989, Value Loss = 0.1374\n",
      "Training batch 24/41\n",
      "Loss = 0.3509, Policy Loss = 0.3020, Value Loss = 0.1354\n",
      "Training batch 25/41\n",
      "Loss = 0.3702, Policy Loss = 0.3214, Value Loss = 0.1318\n",
      "Training batch 26/41\n",
      "Loss = 0.3503, Policy Loss = 0.3016, Value Loss = 0.1281\n",
      "Training batch 27/41\n",
      "Loss = 0.3280, Policy Loss = 0.2794, Value Loss = 0.1261\n",
      "Training batch 28/41\n",
      "Loss = 0.3296, Policy Loss = 0.2811, Value Loss = 0.1226\n",
      "Training batch 29/41\n",
      "Loss = 0.3192, Policy Loss = 0.2707, Value Loss = 0.1199\n",
      "Training batch 30/41\n",
      "Loss = 0.3137, Policy Loss = 0.2653, Value Loss = 0.1171\n",
      "Training batch 31/41\n",
      "Loss = 0.3061, Policy Loss = 0.2578, Value Loss = 0.1143\n",
      "Training batch 32/41\n",
      "Loss = 0.3064, Policy Loss = 0.2582, Value Loss = 0.1119\n",
      "Training batch 33/41\n",
      "Loss = 0.2799, Policy Loss = 0.2318, Value Loss = 0.1093\n",
      "Training batch 34/41\n",
      "Loss = 0.2887, Policy Loss = 0.2407, Value Loss = 0.1061\n",
      "Training batch 35/41\n",
      "Loss = 0.3303, Policy Loss = 0.2824, Value Loss = 0.1045\n",
      "Training batch 36/41\n",
      "Loss = 0.2863, Policy Loss = 0.2385, Value Loss = 0.1007\n",
      "Training batch 37/41\n",
      "Loss = 0.2925, Policy Loss = 0.2447, Value Loss = 0.0981\n",
      "Training batch 38/41\n",
      "Loss = 0.2723, Policy Loss = 0.2246, Value Loss = 0.0963\n",
      "Training batch 39/41\n",
      "Loss = 0.2638, Policy Loss = 0.2162, Value Loss = 0.0939\n",
      "Training batch 40/41\n",
      "Loss = 0.2689, Policy Loss = 0.2214, Value Loss = 0.0907\n",
      "Training batch 41/41\n",
      "Loss = 0.2675, Policy Loss = 0.2201, Value Loss = 0.0883\n",
      "Round 3 completed. Evaluating performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [50:53<00:00, 20.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins for each player: defaultdict(<class 'int'>, {'Player 1': 71, 'Player 2': 79})\n",
      "Winrate of the Target Player: 0.00%; Model is rejected.\n",
      "Round 4: The bots are playing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [14:44<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data prepared.\n",
      "Total samples: 21239, Batches: 41\n",
      "Training batch 1/41\n",
      "Loss = 0.7510, Policy Loss = 0.7002, Value Loss = 0.2145\n",
      "Training batch 2/41\n",
      "Loss = 0.7230, Policy Loss = 0.6723, Value Loss = 0.2099\n",
      "Training batch 3/41\n",
      "Loss = 0.7009, Policy Loss = 0.6503, Value Loss = 0.2060\n",
      "Training batch 4/41\n",
      "Loss = 0.6665, Policy Loss = 0.6159, Value Loss = 0.2025\n",
      "Training batch 5/41\n",
      "Loss = 0.6307, Policy Loss = 0.5802, Value Loss = 0.1996\n",
      "Training batch 6/41\n",
      "Loss = 0.6085, Policy Loss = 0.5582, Value Loss = 0.1956\n",
      "Training batch 7/41\n",
      "Loss = 0.6054, Policy Loss = 0.5552, Value Loss = 0.1916\n",
      "Training batch 8/41\n",
      "Loss = 0.5762, Policy Loss = 0.5260, Value Loss = 0.1891\n",
      "Training batch 9/41\n",
      "Loss = 0.5524, Policy Loss = 0.5024, Value Loss = 0.1842\n",
      "Training batch 10/41\n",
      "Loss = 0.5384, Policy Loss = 0.4884, Value Loss = 0.1797\n",
      "Training batch 11/41\n",
      "Loss = 0.5031, Policy Loss = 0.4531, Value Loss = 0.1780\n",
      "Training batch 12/41\n",
      "Loss = 0.5049, Policy Loss = 0.4551, Value Loss = 0.1747\n",
      "Training batch 13/41\n",
      "Loss = 0.4684, Policy Loss = 0.4187, Value Loss = 0.1695\n",
      "Training batch 14/41\n",
      "Loss = 0.4542, Policy Loss = 0.4045, Value Loss = 0.1665\n",
      "Training batch 15/41\n",
      "Loss = 0.4461, Policy Loss = 0.3965, Value Loss = 0.1632\n",
      "Training batch 16/41\n",
      "Loss = 0.4433, Policy Loss = 0.3938, Value Loss = 0.1582\n",
      "Training batch 17/41\n",
      "Loss = 0.4145, Policy Loss = 0.3651, Value Loss = 0.1560\n",
      "Training batch 18/41\n",
      "Loss = 0.3977, Policy Loss = 0.3484, Value Loss = 0.1534\n",
      "Training batch 19/41\n",
      "Loss = 0.4050, Policy Loss = 0.3558, Value Loss = 0.1510\n",
      "Training batch 20/41\n",
      "Loss = 0.3883, Policy Loss = 0.3392, Value Loss = 0.1464\n",
      "Training batch 21/41\n",
      "Loss = 0.3723, Policy Loss = 0.3232, Value Loss = 0.1430\n",
      "Training batch 22/41\n",
      "Loss = 0.3657, Policy Loss = 0.3167, Value Loss = 0.1398\n",
      "Training batch 23/41\n",
      "Loss = 0.3669, Policy Loss = 0.3180, Value Loss = 0.1369\n",
      "Training batch 24/41\n",
      "Loss = 0.3628, Policy Loss = 0.3140, Value Loss = 0.1325\n",
      "Training batch 25/41\n",
      "Loss = 0.3450, Policy Loss = 0.2962, Value Loss = 0.1295\n",
      "Training batch 26/41\n",
      "Loss = 0.3382, Policy Loss = 0.2895, Value Loss = 0.1284\n",
      "Training batch 27/41\n",
      "Loss = 0.3346, Policy Loss = 0.2861, Value Loss = 0.1238\n",
      "Training batch 28/41\n",
      "Loss = 0.3094, Policy Loss = 0.2609, Value Loss = 0.1210\n",
      "Training batch 29/41\n",
      "Loss = 0.3222, Policy Loss = 0.2738, Value Loss = 0.1182\n",
      "Training batch 30/41\n",
      "Loss = 0.3168, Policy Loss = 0.2685, Value Loss = 0.1147\n",
      "Training batch 31/41\n",
      "Loss = 0.2896, Policy Loss = 0.2413, Value Loss = 0.1122\n",
      "Training batch 32/41\n",
      "Loss = 0.2933, Policy Loss = 0.2452, Value Loss = 0.1086\n",
      "Training batch 33/41\n",
      "Loss = 0.3026, Policy Loss = 0.2545, Value Loss = 0.1063\n",
      "Training batch 34/41\n",
      "Loss = 0.2882, Policy Loss = 0.2402, Value Loss = 0.1030\n",
      "Training batch 35/41\n",
      "Loss = 0.2988, Policy Loss = 0.2509, Value Loss = 0.1004\n",
      "Training batch 36/41\n",
      "Loss = 0.2585, Policy Loss = 0.2107, Value Loss = 0.0991\n",
      "Training batch 37/41\n",
      "Loss = 0.2814, Policy Loss = 0.2337, Value Loss = 0.0958\n",
      "Training batch 38/41\n",
      "Loss = 0.3119, Policy Loss = 0.2643, Value Loss = 0.0938\n",
      "Training batch 39/41\n",
      "Loss = 0.2705, Policy Loss = 0.2229, Value Loss = 0.0903\n",
      "Training batch 40/41\n",
      "Loss = 0.2547, Policy Loss = 0.2072, Value Loss = 0.0884\n",
      "Training batch 41/41\n",
      "Loss = 0.2587, Policy Loss = 0.2114, Value Loss = 0.0860\n",
      "Round 4 completed. Evaluating performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [50:52<00:00, 20.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of wins for each player: defaultdict(<class 'int'>, {'Player 1': 68, 'Player 2': 81, 'Target': 1})\n",
      "Winrate of the Target Player: 0.67%; Model is rejected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Training\n",
    "\n",
    "rl_train(rounds=5, num_games=600, simNum=300, prior=False, ctd_from=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46f6eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T08:51:56.839194Z",
     "iopub.status.busy": "2025-04-16T08:51:56.838453Z",
     "iopub.status.idle": "2025-04-16T08:51:56.842552Z",
     "shell.execute_reply": "2025-04-16T08:51:56.841868Z"
    },
    "papermill": {
     "duration": 0.188547,
     "end_time": "2025-04-16T08:51:56.843966",
     "exception": false,
     "start_time": "2025-04-16T08:51:56.655419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Evaluate Perfromance\n",
    "\n",
    "# game = NoThanksBoard(n_players = 3)\n",
    "# Player_0 = PUCTPlayer(game=game, turn=0, simNum=500)\n",
    "# Player_1 = PUCTPlayer(game=game, turn=1, simNum=500)\n",
    "# Player_2 = PUCTPlayer(game, turn=2, simNum=500)\n",
    "\n",
    "# # model = PolicyValueNet(game.n_players, 32)\n",
    "# # model.load_state_dict(torch.load('policy_value_net_rd0.pth', weights_only=True))\n",
    "# # model.eval()\n",
    "\n",
    "# # model = PolicyOnlyNet(game.n_players, 128)\n",
    "# # model.load_state_dict(torch.load('policy_only_net.pth'))\n",
    "# # model.eval()\n",
    "\n",
    "# # new_prior = smart_prior_fn(game)\n",
    "# # new_prior = nn_prior_fn(model, game)\n",
    "# # Player_0.prior = smart_prior_fn(game)\n",
    "# # Player_1.prior = new_prior\n",
    "# # Player_2.prior = new_prior\n",
    "\n",
    "# players = [Player_0, Player_1, Player_2]\n",
    "\n",
    "# # rl_train(rounds=1, num_games=2, simNum=2000, prior=False, ctd_from=0)\n",
    "\n",
    "# # play(game, players, display=True)\n",
    "\n",
    "# winrate = eval_performance(game, Player_0, [Player_1, Player_2], num_games=300, verbose=True)\n",
    "\n",
    "# print(f\"Winrate of the Target Player: {winrate:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7117104,
     "sourceId": 11369337,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 299798,
     "modelInstanceId": 278893,
     "sourceId": 332791,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19499.756164,
   "end_time": "2025-04-16T08:51:59.318421",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-16T03:26:59.562257",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
