{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is a shortcut for the No thanks program. The policy-ONLY network is trained ONCE on UCT Players' data. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport random\nfrom dataclasses import dataclass, field\n\nimport time\nfrom math import log, sqrt\nfrom collections import deque\n\nimport sys\nimport json\nfrom multiprocess import Pool\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:30.458946Z","iopub.execute_input":"2025-04-11T21:25:30.459284Z","iopub.status.idle":"2025-04-11T21:25:34.997682Z","shell.execute_reply.started":"2025-04-11T21:25:30.459261Z","shell.execute_reply":"2025-04-11T21:25:34.996742Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"## Progress bar\n\ndef log_progress(sequence, every=None, size=None, name='Items'):\n    from ipywidgets import IntProgress, HTML, VBox\n    from IPython.display import display\n\n    is_iterator = False\n    if size is None:\n        try:\n            size = len(sequence)\n        except TypeError:\n            is_iterator = True\n    if size is not None:\n        if every is None:\n            if size <= 200:\n                every = 1\n            else:\n                every = int(size / 200)     # every 0.5%\n    else:\n        assert every is not None, 'sequence is iterator, set every'\n\n    if is_iterator:\n        progress = IntProgress(min=0, max=1, value=1)\n        progress.bar_style = 'info'\n    else:\n        progress = IntProgress(min=0, max=size, value=0)\n    label = HTML()\n    box = VBox(children=[label, progress])\n    display(box)\n\n    index = 0\n    try:\n        for index, record in enumerate(sequence, 1):\n            if index == 1 or index % every == 0:\n                if is_iterator:\n                    label.value = '{name}: {index} / ?'.format(\n                        name=name,\n                        index=index\n                    )\n                else:\n                    progress.value = index\n                    label.value = u'{name}: {index} / {size}'.format(\n                        name=name,\n                        index=index,\n                        size=size\n                    )\n            yield record\n    except:\n        progress.bar_style = 'danger'\n        raise\n    else:\n        progress.bar_style = 'success'\n        progress.value = index\n        label.value = \"{name}: {index}\".format(\n            name=name,\n            index=str(index or '?')\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:34.999233Z","iopub.execute_input":"2025-04-11T21:25:35.000105Z","iopub.status.idle":"2025-04-11T21:25:35.008781Z","shell.execute_reply.started":"2025-04-11T21:25:35.000067Z","shell.execute_reply":"2025-04-11T21:25:35.007804Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## Network\n\nclass PolicyOnlyNet(nn.Module):\n    def __init__(self, n_players, hidden_dim):\n        super(PolicyOnlyNet, self).__init__()\n\n        # CNN Branch for processing matrix M\n        self.cnn_branch = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(8),  # Batch Norm after Conv\n            nn.ReLU(),\n            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),  # Batch Norm after Conv\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((4, 4)),  # Downsampling to a fixed size\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 4, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Fully-connected Branch for processing vector b\n        self.fc_branch = nn.Sequential(\n            nn.Linear(3, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Main Branch\n        self.main_branch = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n            nn.BatchNorm1d(hidden_dim * 2),  # Batch Norm after Linear\n            nn.ReLU(),\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Policy Head\n        self.policy_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()  # Output in range [0, 1]\n        )\n\n        # Value Head\n        # self.value_head = nn.Sequential(\n        #     nn.Linear(hidden_dim, hidden_dim),\n        #     nn.BatchNorm1d(hidden_dim), \n        #     nn.ReLU(),\n        #     nn.Linear(hidden_dim, hidden_dim),\n        #     nn.BatchNorm1d(hidden_dim), \n        #     nn.ReLU(),\n        #     nn.Linear(hidden_dim, hidden_dim),\n        #     nn.BatchNorm1d(hidden_dim), \n        #     nn.ReLU(),\n        #     nn.Linear(hidden_dim, 1),\n        #     nn.Sigmoid()   # Output n-dim vector in range [0, 1]\n        # )\n\n    def forward(self, M, b):\n        M = self.cnn_branch(M)\n        b = self.fc_branch(b)\n        x = torch.cat((M, b), dim=-1)\n        x = self.main_branch(x)\n        p = self.policy_head(x)\n        # value = self.value_head(x)\n        # print(f\"Debug: Policy Head Output (p): {p}\")  # Debugging output\n        return p\n\ndef train_nn(model, batch_size, n_players, hidden_dim, input_state, target_policy):\n    M, b = input_state\n    M = torch.tensor(M, dtype=torch.float32)\n    b = torch.tensor(b, dtype=torch.float32)\n    target_policy = torch.tensor(target_policy, dtype=torch.float32).view(-1, 1)\n    # target_value = torch.tensor(target_value, dtype=torch.float32).view(-1, 1)\n\n\n    # Define optimizer and loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n    weights = torch.where(target_policy == 1., torch.tensor(2.0), torch.tensor(1.0))  # Adjust this value as needed\n    policy_loss_fn = nn.BCELoss(weight=weights)  # Binary Cross-Entropy Loss with positive weight\n    # value_loss_fn = nn.MSELoss()\n\n    # Training loop (few steps)\n    for step in range(10):\n        optimizer.zero_grad()\n\n        # Forward pass\n        pred_policy = model(M, b)\n\n        # Compute policy loss\n        policy_loss = policy_loss_fn(pred_policy, target_policy)\n    \n        # Compute value loss\n        # value_loss = value_loss_fn(pred_value, target_value)\n\n        # Compute L2 regularization term (sum of all parameters' squared values)\n        l2_lambda = 1e-4\n        l2_penalty = sum(w.pow(2.0).sum() for w in model.parameters())\n\n        # Combine losses\n        loss = policy_loss + l2_lambda * l2_penalty\n\n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n        optimizer.step()\n\n        if step == 9:\n            print(f\"Loss = {loss.item():.4f}, \"\n                  f\"Policy Loss = {policy_loss.item():.4f}\"\n                  )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:35.009827Z","iopub.execute_input":"2025-04-11T21:25:35.010474Z","iopub.status.idle":"2025-04-11T21:25:35.030548Z","shell.execute_reply.started":"2025-04-11T21:25:35.010443Z","shell.execute_reply":"2025-04-11T21:25:35.029774Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## Game\n\nACTION_TAKE = 0\nACTION_PASS = 1\n\nrandom.seed(999)\n\ndef diff(first, second):\n    second = set(second)\n    return [item for item in first if item not in second]\n\n@dataclass\nclass NoThanksConfig:\n    min_card: int = 3\n    max_card: int = 35\n    n_omit_cards: int = 9\n    n_players: int = 3\n    start_coins: int = field(init=False)\n\n    def __post_init__(self):\n        self.start_coins = self.calculate_start_coins()\n\n    def calculate_start_coins(self):\n        if 3 <= self.n_players <= 5:\n            return 11\n        elif self.n_players == 6:\n            return 9\n        elif self.n_players == 7:\n            return 7\n        else:\n            raise ValueError(\"Number of players must be between 3 and 7\")\n\nclass NoThanksBoard():\n    def __init__(self, n_players = 3, config = NoThanksConfig(n_players=3)):\n        self.n_players = n_players\n        self.min_card = config.min_card\n        self.max_card = config.max_card\n        self.full_deck = list(range(self.min_card, self.max_card+1))\n        self.n_omit_cards = config.n_omit_cards\n        self.n_cards = self.max_card - self.min_card + 1\n        self.start_coins = config.start_coins\n        random.seed(999)\n    \n    def reward_dict(self):\n        if self.n_players == 3:\n            return {1: 1, 2: 0, 3: -1}\n        elif self.n_players == 4:\n            return {1: 1, 2: 0.5, 3: -0.5, 4: -1}\n        elif self.n_players == 5:\n            return {1: 1, 2: 0.5, 3: 0, 4: -0.5, 5: -1}\n        elif self.n_players == 6:\n            return {1: 1, 2: 0.75, 3: 0.5, 4: -0.5, 5: -0.75, 6: -1}\n        elif self.n_players == 7:\n            return {1: 1, 2: 0.75, 3: 0.5, 4: 0, 5: -0.5, 6: -0.75, 7: -1}\n\n            \n    # state: ((player coins),(player cards),(card in play, coins in play, n_cards_remaining, current player))\n    def starting_state(self, current_player = 0):\n        coins = [self.start_coins for i in range(self.n_players)]\n        cards = [[] for i in range(self.n_players)]\n\n        card_in_play = random.choice(self.full_deck)\n        \n        coins_in_play = 0\n        n_cards_in_deck = self.n_cards - 1 - self.n_omit_cards\n\n        return coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n\n    def next_state(self, state, action):\n\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if action == ACTION_TAKE:\n            cards[current_player].append(card_in_play)\n            coins[current_player] += coins_in_play\n\n            all_player_cards = [card for player_cards in cards for card in player_cards]\n            cards_in_deck = diff(self.full_deck, all_player_cards)\n            current_player = current_player\n            \n            if cards_in_deck and n_cards_in_deck > 0:   \n                # random.shuffle(list(cards_in_deck))\n                card_in_play = random.choice(cards_in_deck)\n                n_cards_in_deck -= 1\n            else:\n                card_in_play = None\n            coins_in_play = 0\n\n        else:\n            coins[current_player] -= 1\n            coins_in_play += 1\n            current_player += 1\n        \n        if current_player == self.n_players:\n            current_player = 0\n\n        next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n        return self.pack_state(next_state)\n    \n    def all_possible_next(self, state, action):\n        if action == ACTION_PASS:\n            return self.next_state(state, action)\n        elif action == ACTION_TAKE:\n            next_states = []\n            state = self.unpack_state(state)\n            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n            cards[current_player].append(card_in_play)\n            coins[current_player] += coins_in_play\n            n_cards_in_deck -= 1\n            coins_in_play = 0\n\n            all_player_cards = [card for player_cards in cards for card in player_cards]\n            cards_in_deck = diff(self.full_deck, all_player_cards)\n            current_player = current_player\n            \n            if not cards_in_deck:\n                return self.next_state(state, action)\n            else:\n                for card in cards_in_deck: \n                    card_in_play = card\n                    if current_player == self.n_players:\n                        current_player = 0\n                    next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n                    next_state = self.pack_state(next_state)\n                    next_states.append(next_state)\n            \n            return next_states\n\n    def is_legal(self, state, action):\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if card_in_play is None:\n            return False\n        if coins[current_player] <= 0 and action == ACTION_PASS:\n            return False\n        else:\n            return True\n\n    def legal_actions(self, state):\n        actions = []\n        \n        if self.is_legal(state, ACTION_TAKE):\n            actions.append(ACTION_TAKE)\n\n        if self.is_legal(state, ACTION_PASS):\n            actions.append(ACTION_PASS)\n\n        return actions\n\n    def pack_state(self, state):\n        coins, cards, details = state\n        packed_state = tuple(coins), tuple(map(tuple, cards)), details\n        return packed_state\n\n    def unpack_state(self, packed_state):\n        coins, cards, details = packed_state\n        coins = list(coins)\n        cards = list(map(list, cards))\n        return coins, cards, details\n    \n\n    def standard_state(self, state):\n        \"\"\"\n        Input state (packed or unpacked): ([coins], [[cards]], (card_in_play, coins_in_play, n_cards_in_deck, current_player))\n        Transform state into the required format:\n        1. Extract the state into M and b where M is the card/coin matrix and b is the vector (card_in_play, coins_in_play, n_cards_in_deck)\n        2. Rotate M such that the first row corresponds to the current player\n        3. Transform M into array of shape (3, n_players, 33)\n        \"\"\"\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n        \n        # Step 1: Build the M matrix (n_players x 34)\n        M = []\n        for k in range(self.n_players):\n            # Initialize the card representation for player k\n            card_rep = [0] * 33  # 33 cards\n\n            # Set 1 for each card player k has\n            for card in cards[k]:\n                card_rep[card - 3] = 1  # Cards are indexed from 3 to 35\n\n            # Add the number of coins for player k\n            M.append(card_rep + [coins[k]])  # 33 card columns + 1 coin column\n        \n        # Step 2: Build the b vector (card_in_play, coins_in_play, n_cards_in_deck)\n        b = np.array([card_in_play, coins_in_play, n_cards_in_deck])\n\n        # Step 3: Rotate M such that the first row is the current player\n        M_rotated = M[current_player:] + M[:current_player]  # Rotate the matrix\n        \n        # Step 4: Transform M into array of shape (3, n_players, 33)\n        # where M[0] is the card matrix, M[1] is the coin matrix, M[2] is the card in play\n        M_transformed = np.zeros((3, self.n_players, 33))\n        M_rotated = np.array(M_rotated)\n        M_transformed[0] = M_rotated[:, :-1]  # Card matrix\n        M_transformed[1] = np.repeat(M_rotated[:, -1][:, np.newaxis], 33, axis=1)  # Coin matrix\n        M_transformed[1] = M_transformed[1] / (self.start_coins * self.n_players)  # Normalize coins to be between 0 and 1\n        \n        card_in_play_onehot = np.zeros(33)\n        if 3 <= card_in_play <= 35:\n            card_in_play_onehot[card_in_play - 3] = 1\n        M_transformed[2] = np.tile(card_in_play_onehot, (self.n_players, 1))\n\n        return M_transformed, b\n\n\n    def is_ended(self, state):\n        # print(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if n_cards_in_deck == 0 and card_in_play == None:\n            return True\n        else:\n            return False\n\n    def compute_scores(self, state):\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        scores = []\n\n        for p_idx in range(self.n_players):\n            cards[p_idx].sort()\n\n            score = 0\n            if cards[p_idx]:\n                score += cards[p_idx][0]\n                last_card = cards[p_idx][0]\n\n                for card_idx in range(1, len(cards[p_idx])):\n                    new_card = cards[p_idx][card_idx]\n\n                    if not new_card == last_card + 1:\n                        score += new_card\n                    last_card = new_card\n\n            score -= coins[p_idx]\n\n            scores.append(score)\n\n        return scores\n\n    def winner(self, state):\n        \"\"\"Temporary winner: player with the lowest score even if the game is not ended.\"\"\"\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if not self.is_ended(state):\n            return None\n        \n        scores = self.compute_scores(state)\n        min_score = 1000\n        lowest_scorers = []\n        # get lowest scorers (could be more than one)\n        for i, score in enumerate(scores):\n            if score < min_score:\n                lowest_scorers = [i]\n                min_score = score\n            if score <= min_score:\n                lowest_scorers.append(i)\n        \n        # if players are tied on lowest score, get the one with the fewest cards\n        if len(lowest_scorers) > 1:\n            min_n_cards = 1000\n            for i in lowest_scorers:\n                n_cards = len(cards[i])\n                if n_cards < min_n_cards:\n                    lowest_card_players = [i]\n                    min_n_cards = n_cards\n                elif n_cards <= min_n_cards:\n                    lowest_card_players.append(i)\n\n            if len(lowest_card_players) > 1:\n                winner = lowest_card_players[0]\n            else: # if still tied, pick a random winner (not the official rules)\n                winner = random.choice(lowest_card_players) \n        else:\n            winner = lowest_scorers[0]\n\n        return winner\n    \n    def reward_rank(self, state):\n        state = self.unpack_state(state)\n        scores = self.compute_scores(state)\n        rank = sorted(range(len(scores)), key=lambda k: scores[k])\n        value = [self.reward_dict()[rank.index(player) + 1] for player in range(self.n_players)]\n        return np.array(value)\n    \n    def reward_winloss(self, state):\n        state = self.unpack_state(state)\n        value = [1 if self.winner(state) == player else -1 for player in range(self.n_players)]\n        return np.array(value)\n\n    def reward_score(self, state):\n        state = self.unpack_state(state)\n        scores = self.compute_scores(state)\n        rewards = [-score for score in scores]\n\n        return np.array(rewards)\n\n    \n    def basic_display_state(self, state):\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        print(\"Coins:           {0}\".format(coins))\n        print(\"Cards:           {0}\".format(cards))\n        print(\"Card in play:    {0}\".format(card_in_play))\n        print(\"Coins:           {0}\".format(coins_in_play))\n        print(\"Player:          {0}\".format(current_player))\n\n    def display_scores(self, state):\n        scores = self.compute_scores(state)\n        print(\"\")\n        print(\"--- Scores ---\")\n        for i in range(self.n_players):\n            print(\"Player {0}: {1}\".format(i, scores[i]))\n        print(\"\")\n\n    def display_state(self, state, human_player=None):\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        scores = self.compute_scores(state)\n\n        def format_cards(card_list):\n            return \", \".join(map(str, sorted(card_list)))\n\n        player_labels = [f\"Player {i}\" + (\" (You)\" if i == human_player else \"\") for i in range(self.n_players)]\n        card_strings = [format_cards(cards[i]) for i in range(self.n_players)]\n        coin_strings = [str(coins[i]) for i in range(self.n_players)]\n        score_strings = [str(scores[i]) for i in range(self.n_players)]\n\n        max_card_len = max(20, max(len(card_str) for card_str in card_strings))\n\n        print(\"\")\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n        print(\"\")\n        print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\"Player\", \"Cards\", max_card_len, \"Coins\", \"Score\"))\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n\n        for i in range(self.n_players):\n            print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\n                player_labels[i],\n                card_strings[i],\n                max_card_len,\n                coin_strings[i],\n                score_strings[i]\n            ))\n\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n        print(\"\\t\\t In play: [{0}]\".format(card_in_play))\n        print(\"\\t\\t Cards remaining: {0}\".format(n_cards_in_deck))\n        print(\"\\t\\t   Coins: {0}\".format(coins_in_play))\n        print(\"\")\n            \n    def pack_action(self, notation):\n        if notation == \"y\" or notation == \"Y\":\n            return ACTION_TAKE\n        else:\n            return ACTION_PASS\n\n    def current_player(self, state):\n        return state[2][3]\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:35.032184Z","iopub.execute_input":"2025-04-11T21:25:35.032451Z","iopub.status.idle":"2025-04-11T21:25:35.074241Z","shell.execute_reply.started":"2025-04-11T21:25:35.032426Z","shell.execute_reply":"2025-04-11T21:25:35.073213Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"## Players\n\nclass Player:\n    \"\"\"The abstract class for a player. A player can be an AI agent (bot) or human.\"\"\"\n    def __init__(self, name, game, turn):\n        self.name = name\n        self.game = game\n        self.turn = turn # starting form 0 as convention in python\n        assert self.turn < self.game.n_players, \"Player turn out of range.\"\n\n    def get_action(self, state):\n        raise NotImplementedError\n    \nclass UCTPlayer(Player):\n    \"\"\"Monte Carlo Tree Search Player (UCT, no prior)\"\"\"\n    def __init__(self, game, thinking_time=1, turn=0):\n        assert thinking_time > 0\n        self.turn = turn\n        self.game = game\n        self.thinking_time = thinking_time\n        self.max_moves = 200\n        self.C = 1.4  # Exploration parameter for UCB1\n        self.max_depth = 0\n\n    def get_action(self, state):\n        board = self.game\n        player = board.current_player(state)\n        legal_actions = board.legal_actions(state)\n        \n        if not legal_actions:\n            return None, None\n        if len(legal_actions) == 1:\n            return legal_actions[0], 0\n        \n        # Initialize visit and win counts\n        plays, wins = {}, {}\n        games = 0\n        start_time = time.perf_counter()\n\n        # Run MCTS for the specified thinking time\n        while time.perf_counter() - start_time < self.thinking_time:\n            self.run_simulation(state, board, plays, wins)\n            games += 1\n\n        # Choose the best action based on win rate\n        random.shuffle(legal_actions)\n        action = max(\n            legal_actions,\n            key=lambda a: plays.get((player, state, a), 1)\n        )\n        # print(\"UCT:\", 0, wins.get((player, state, 0), 0) / plays.get((player, state, 0), 1))\n        # print(\"UCT:\", 1, wins.get((player, state, 1), 0) / plays.get((player, state, 1), 1))\n        # print(\"Player:\", player, \"Max depth searched:\", self.max_depth, \"Games played:\", games)\n        return action, wins.get((player, state, action), 0) / plays.get((player, state, action), 1)\n\n    def run_simulation(self, state, board, plays, wins):\n        \"\"\"Run a single MCTS simulation.\"\"\"\n        tree = set()\n        player = board.current_player(state)\n\n        # === Selection & Expansion ===\n        for t in range(1, self.max_moves + 1):\n            legal_actions = board.legal_actions(state)\n\n            # Selection using UCB1 if data exists for all actions\n            if all(plays.get((player, state, a)) for a in legal_actions):\n                log_total = log(sum(plays[(player, state, a)] for a in legal_actions))\n                action = max(\n                    legal_actions,\n                    key=lambda a: (\n                        wins[(player, state, a)] / plays[(player, state, a)] +\n                        self.C * sqrt(log_total / plays[(player, state, a)])\n                    )\n                )\n                \n            else:\n                # Expansion – If any action is unexplored, take a random one\n                action = random.choice(legal_actions)\n                if (player, state, action) not in plays:\n                    plays[(player, state, action)] = 0\n                    wins[(player, state, action)] = 0\n                    if t > self.max_depth:\n                        self.max_depth = t\n\n            tree.add((player, state, action))\n            state = board.next_state(state, action)\n            player = board.current_player(state)\n\n            # Check for game-ending state\n            winner = board.winner(state)\n            if winner is not None:\n                break\n\n        # === Backpropagation ===\n        for player, state, action in tree:\n            plays[(player, state, action)] += 1\n            if player == winner:\n                wins[(player, state, action)] += 1\n    \nclass PUCTPlayer(Player):\n    \"\"\"Monte Carlo Tree Search Player (Prior given by NN)\"\"\"\n    def __init__(self, game, thinking_time=1, turn=0):\n        assert thinking_time > 0\n        self.turn = turn\n        self.game = game\n        self.thinking_time = thinking_time\n        self.max_moves = 200\n        self.C = 4  # Exploration parameter for PUCT\n        self.max_depth = 0\n        self.prior = lambda state, action: 1 / len(self.game.legal_actions(state))  # Default prior\n        self.value = None\n\n    def get_action(self, state):\n        board = self.game\n        player = board.current_player(state)\n        legal_actions = board.legal_actions(state)\n        \n        if not legal_actions:\n            return None, None\n        if len(legal_actions) == 1:\n            return legal_actions[0], 0\n        \n        # Initialize visit and win counts\n        plays, wins = {}, {}\n        games = 0\n        start_time = time.perf_counter()\n\n        # Run MCTS for the specified thinking time\n        # while time.perf_counter() - start_time < self.thinking_time:\n        #     self.run_simulation(state, board, plays, wins)\n        #     games += 1\n\n        for _ in range(1000):\n            self.run_simulation(state, board, plays, wins)\n            games += 1\n\n        # Choose the best action based on win rate\n        random.shuffle(legal_actions)\n        action = max(\n            legal_actions,\n            key=lambda a: ( # choose the action with highest visits\n                plays.get((player, state, a), 1)\n            )\n        )\n\n        # print(\"PUCT:\", 0, wins.get((player, state, 0), 0) / plays.get((player, state, 0), 1))\n        # print(\"PUCT:\", 1, wins.get((player, state, 1), 0) / plays.get((player, state, 1), 1))\n        # print(\"Player:\", player, \"Max depth searched:\", self.max_depth, \"Games played:\", games)\n        return action, wins.get((player, state, action), 0) / plays.get((player, state, action), 1)\n\n    def run_simulation(self, state, board, plays, wins):\n        \"\"\"Run a single MCTS simulation.\"\"\"\n        tree = set()\n        player = board.current_player(state)\n\n        # === Selection & Expansion ===\n        for t in range(1, self.max_moves + 1):\n            legal_actions = board.legal_actions(state)\n\n            # Selection using UCB1 if data exists for all actions\n            if all(plays.get((player, state, a)) for a in legal_actions):\n                total = sum(plays[(player, state, a)] for a in legal_actions)\n                # for a in legal_actions:\n                #     print(\"Check Total:\", a, wins[(player, state, a)] / plays[(player, state, a)])\n                action = max(\n                    legal_actions,\n                    key=lambda a: (\n                        wins[(player, state, a)] / plays[(player, state, a)] +\n                        self.C * self.prior(state, a) * sqrt(total / plays[(player, state, a)])\n                    )\n                )\n            else:\n                # Expansion – If any action is unexplored, take a random one\n                action = random.choice(legal_actions)\n                if (player, state, action) not in plays:\n                    plays[(player, state, action)] = 0\n                    wins[(player, state, action)] = 0\n                    if t > self.max_depth:\n                        self.max_depth = t\n\n            tree.add((player, state, action)) # trajectory\n            state = board.next_state(state, action)\n            player = board.current_player(state)\n\n            # Check for game-ending state\n            winner = board.winner(state)\n            if winner is not None:\n                break\n\n        # === Backpropagation ===\n        for player, state, action in tree:\n            plays[(player, state, action)] += 1\n            if player == winner:\n                wins[(player, state, action)] += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:35.075550Z","iopub.execute_input":"2025-04-11T21:25:35.075840Z","iopub.status.idle":"2025-04-11T21:25:35.101048Z","shell.execute_reply.started":"2025-04-11T21:25:35.075817Z","shell.execute_reply":"2025-04-11T21:25:35.100079Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def self_play(game, players, times=1, to_file=None):\n    data = {\"state\": [], \"policy\": []}\n    for _ in log_progress(range(times)):\n        state = game.starting_state(current_player=0)\n        state = game.pack_state(state)\n        current_player = 0\n\n        while not game.is_ended(state):\n            player = players[current_player]\n            action, _ = player.get_action(state)\n            state = game.next_state(state, action)\n            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = game.unpack_state(state)\n            if card_in_play is not None:\n                data[\"state\"].append(game.standard_state(state))  # Append (M, b) as NumPy arrays\n                data[\"policy\"].append(action)\n                # data[\"value\"].append(score)\n\n    # Convert NumPy arrays in \"state\" to lists for JSON serialization\n    if to_file is not None:\n        serializable_data = {\n            \"state\": [(M.tolist(), b.tolist()) for M, b in data[\"state\"]],\n            \"policy\": data[\"policy\"]\n            # \"value\": data[\"value\"]\n        }\n        with open(to_file, \"w\") as f:\n            json.dump(serializable_data, f)\n\n    return data\n\ndef parallel_self_play(args):\n    game, players, times, file_prefix, process_id = args\n    # time.sleep(0.5)\n    # print(f\"Process {process_id} started.\")\n    return self_play(game, players, times, to_file=f\"{file_prefix}_process{process_id}.json\")\n\ndef parallel_self_play_nosave(args):\n    game, players, times, process_id = args\n    time.sleep(0.25)\n    print(f\"Process {process_id} started.\")\n    return self_play(game, players, times)\n\n\ndef rl_train_nosave(from_file=None, num_processes=4, num_games=4, prior=False):\n    game = NoThanksBoard(n_players=3)\n    Player_0 = UCTPlayer(game=game, turn=0)\n    Player_1 = UCTPlayer(game=game, turn=1)\n    Player_2 = UCTPlayer(game=game, turn=2)\n    players = [Player_0, Player_1, Player_2]\n\n    batch_size = 32\n    n_players = 3\n    model = PolicyOnlyNet(n_players, hidden_dim=128)\n\n    if prior == True:\n        model.load_state_dict(torch.load('policy_only_net.pth', weights_only=True))\n\n\n    # Parallelize self_play\n    games_per_process = num_games // num_processes\n    args = [(game, players, games_per_process, process_id) for process_id in range(num_processes)]\n\n    # Use Pool to collect data from all processes\n    with Pool(num_processes) as pool:\n        results = pool.map(parallel_self_play_nosave, args)\n\n    # Combine results from all processes\n    data = {\"state\": [], \"policy\": []}\n    for result in results:\n        data[\"state\"].extend(result[\"state\"])\n        data[\"policy\"].extend(result[\"policy\"])\n        # data[\"value\"].extend(result[\"value\"])\n\n    # Combine and shuffle the data\n    combined_data = list(zip(data[\"state\"], data[\"policy\"]))\n    random.shuffle(combined_data)\n    data[\"state\"], data[\"policy\"] = zip(*combined_data)\n\n    states = data[\"state\"]\n    target_policy = np.array(data[\"policy\"])\n    # target_value = np.array(data[\"value\"])\n    print(\"Training Data prepared.\")\n    num_samples = len(states)\n    num_batches = num_samples // batch_size\n    print(f\"Total samples: {num_samples}, Batches: {num_batches}\")\n        \n    for batch_idx in range(num_batches):\n        # Extract batch data\n        batch_start = batch_idx * batch_size\n        batch_end = batch_start + batch_size\n        \n        batch_states = states[batch_start:batch_end]\n        batch_policies = target_policy[batch_start:batch_end]\n        # batch_values = target_value[batch_start:batch_end]\n        \n        # Reshape states into (M, b) format\n        M = np.array([s[0] for s in batch_states])\n        b = np.array([np.array(s[1], dtype=np.float32) for s in batch_states], dtype=np.float32)\n\n        # Train the model on the batch\n        model.train()\n        print(f\"Training batch {batch_idx + 1}/{num_batches}\")\n        train_nn(model, batch_size, n_players, 128, (M, b), batch_policies)\n            \n            # Update prior\n            # model.eval()\n            # for player in players:\n            #     player.prior = lambda state, action: (\n            #         model(\n            #             torch.tensor(game.standard_state(state)[0], dtype=torch.float32).unsqueeze(0),\n            #             torch.tensor(game.standard_state(state)[1], dtype=torch.float32).unsqueeze(0)\n            #         )[0].item() if action == ACTION_PASS else 1 - model(\n            #             torch.tensor(game.standard_state(state)[0], dtype=torch.float32).unsqueeze(0),\n            #             torch.tensor(game.standard_state(state)[1], dtype=torch.float32).unsqueeze(0)\n            #         )[0].item()\n            #     )\n\n        torch.save(model.state_dict(), f'policy_only_net.pth')\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:35.102147Z","iopub.execute_input":"2025-04-11T21:25:35.102442Z","iopub.status.idle":"2025-04-11T21:25:35.123727Z","shell.execute_reply.started":"2025-04-11T21:25:35.102409Z","shell.execute_reply":"2025-04-11T21:25:35.122799Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"rl_train_nosave(from_file=None, num_processes=4, num_games=800)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T21:25:35.124575Z","iopub.execute_input":"2025-04-11T21:25:35.124819Z"}},"outputs":[{"name":"stdout","text":"Process 0 started.\nProcess 1 started.\nProcess 2 started.\nProcess 3 started.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9281d17c900746638c0c48df3d9f2442"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29934624aed426a9a736807447f1db5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0b86fc5c61472c9fb049131220e9b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=''), IntProgress(value=0, max=200)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15eb71115fe44626af7ba43fa662d81b"}},"metadata":{}}],"execution_count":null}]}