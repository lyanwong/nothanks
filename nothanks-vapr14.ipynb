{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11369337,"sourceType":"datasetVersion","datasetId":7117104},{"sourceId":332791,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":278893,"modelId":299798}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport random\nfrom dataclasses import dataclass\n\nimport time\nfrom math import log, sqrt\nfrom collections import defaultdict\nfrom abc import ABC, abstractmethod\nfrom itertools import chain\n\nimport sys\nimport json\nimport copy\n\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:04:55.297623Z","iopub.execute_input":"2025-04-15T19:04:55.298002Z","iopub.status.idle":"2025-04-15T19:05:00.909142Z","shell.execute_reply.started":"2025-04-15T19:04:55.297968Z","shell.execute_reply":"2025-04-15T19:05:00.908310Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"## Progress bar\n\ndef log_progress(sequence, every=None, size=None, name='Items'):\n    from ipywidgets import IntProgress, HTML, VBox\n    from IPython.display import display\n\n    is_iterator = False\n    if size is None:\n        try:\n            size = len(sequence)\n        except TypeError:\n            is_iterator = True\n    if size is not None:\n        if every is None:\n            if size <= 200:\n                every = 1\n            else:\n                every = int(size / 200)     # every 0.5%\n    else:\n        assert every is not None, 'sequence is iterator, set every'\n\n    if is_iterator:\n        progress = IntProgress(min=0, max=1, value=1)\n        progress.bar_style = 'info'\n    else:\n        progress = IntProgress(min=0, max=size, value=0)\n    label = HTML()\n    box = VBox(children=[label, progress])\n    display(box)\n\n    index = 0\n    try:\n        for index, record in enumerate(sequence, 1):\n            if index == 1 or index % every == 0:\n                if is_iterator:\n                    label.value = '{name}: {index} / ?'.format(\n                        name=name,\n                        index=index\n                    )\n                else:\n                    progress.value = index\n                    label.value = u'{name}: {index} / {size}'.format(\n                        name=name,\n                        index=index,\n                        size=size\n                    )\n            yield record\n    except:\n        progress.bar_style = 'danger'\n        raise\n    else:\n        progress.bar_style = 'success'\n        progress.value = index\n        label.value = \"{name}: {index}\".format(\n            name=name,\n            index=str(index or '?')\n        )\n\nbar = tqdm # tqdm if in terminal","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:00.910617Z","iopub.execute_input":"2025-04-15T19:05:00.911083Z","iopub.status.idle":"2025-04-15T19:05:00.920855Z","shell.execute_reply.started":"2025-04-15T19:05:00.911048Z","shell.execute_reply":"2025-04-15T19:05:00.919849Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"## Network\n\nclass PolicyValueNet(nn.Module):\n    def __init__(self, n_players, hidden_dim):\n        super(PolicyValueNet, self).__init__()\n\n        # CNN Branch for processing matrix M\n        self.cnn_branch = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(8),  # Batch Norm after Conv\n            nn.ReLU(),\n            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),  # Batch Norm after Conv\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((4, 4)),  # Downsampling to a fixed size\n            nn.Flatten(),\n            nn.Linear(32 * 4 * 4, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Fully-connected Branch for processing vector b\n        self.fc_branch = nn.Sequential(\n            nn.Linear(3, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Main Branch\n        self.main_branch = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n            nn.BatchNorm1d(hidden_dim * 2),  # Batch Norm after Linear\n            nn.ReLU(),\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),  # Batch Norm after Linear\n            nn.ReLU()\n        )\n\n        # Policy Head\n        self.policy_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()  # Output in range [0, 1]\n        )\n\n        # Value Head\n        self.value_head = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()   # Output n-dim vector in range [0, 1]\n        )\n\n    def forward(self, M, b):\n        M = self.cnn_branch(M)\n        b = self.fc_branch(b)\n        x = torch.cat((M, b), dim=-1)\n        x = self.main_branch(x)\n        p = self.policy_head(x)\n        value = self.value_head(x)\n        # print(f\"Debug: Policy Head Output (p): {p}\")  # Debugging output\n        return p, value\n\ndef train_nn(model, batch_size, n_players, hidden_dim, input_state, target_policy, target_value):\n    M, b = input_state\n    M = torch.tensor(M, dtype=torch.float32)\n    b = torch.tensor(b, dtype=torch.float32)\n    target_policy = torch.tensor(target_policy, dtype=torch.float32).view(-1, 1)\n    target_value = torch.tensor(target_value, dtype=torch.float32).view(-1, 1)\n\n\n    # Define optimizer and loss\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n    weights = torch.where(target_policy == 1., torch.tensor(1.1), torch.tensor(1.0))  # Adjust this value as needed\n    policy_loss_fn = nn.BCELoss(weight=weights)  # Binary Cross-Entropy Loss with positive weight\n    value_loss_fn = nn.MSELoss()\n\n    # Training loop (few steps)\n    for step in range(10):\n        optimizer.zero_grad()\n\n        # Forward pass\n        pred_policy, pred_value = model(M, b)\n\n        # Compute policy loss\n        policy_loss = policy_loss_fn(pred_policy, target_policy)\n    \n        # Compute value loss\n        value_loss = value_loss_fn(pred_value, target_value)\n\n        # Compute L2 regularization term (sum of all parameters' squared values)\n        l2_lambda = 1e-4\n        l2_penalty = sum(w.pow(2.0).sum() for w in model.parameters())\n\n        # Combine losses\n        loss = policy_loss + value_loss*0.01 + l2_lambda * l2_penalty\n\n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Clip gradients\n        optimizer.step()\n\n        if step == 9:\n            print(f\"Loss = {loss.item():.4f}, \"\n                  f\"Policy Loss = {policy_loss.item():.4f}, \"\n                  f\"Value Loss = {value_loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:00.922200Z","iopub.execute_input":"2025-04-15T19:05:00.923289Z","iopub.status.idle":"2025-04-15T19:05:00.951367Z","shell.execute_reply.started":"2025-04-15T19:05:00.923253Z","shell.execute_reply":"2025-04-15T19:05:00.950236Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"## Game\n\nACTION_TAKE = 0\nACTION_PASS = 1\n\ndef diff(first, second):\n    second = set(second)\n    return [item for item in first if item not in second]\n\n@dataclass\nclass NoThanksConfig:\n    min_card: int = 3\n    max_card: int = 14\n    n_omit_cards: int = 3\n    n_players: int = 3\n    start_coins: int = 4\n\n\nclass NoThanksBoard():\n    def __init__(self, n_players = 3, config = NoThanksConfig):\n        self.n_players = n_players\n        self.min_card = config.min_card\n        self.max_card = config.max_card\n        self.full_deck = list(range(self.min_card, self.max_card+1))\n        self.n_omit_cards = config.n_omit_cards\n        self.n_cards = self.max_card - self.min_card + 1\n        self.start_coins = config.start_coins\n        random.seed(999)\n\n            \n    # state: ((player coins),(player cards),(card in play, coins in play, n_cards_remaining, current player))\n    def starting_state(self, current_player = 0):\n        coins = [self.start_coins for i in range(self.n_players)]\n        cards = [[] for i in range(self.n_players)]\n\n        card_in_play = random.choice(self.full_deck)\n        \n        coins_in_play = 0\n        n_cards_in_deck = self.n_cards - 1 - self.n_omit_cards\n\n        return coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n\n    def next_state(self, state, action):\n\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if action == ACTION_TAKE:\n            cards[current_player].append(card_in_play)\n            coins[current_player] += coins_in_play\n\n            all_player_cards = [card for player_cards in cards for card in player_cards]\n            cards_in_deck = diff(self.full_deck, all_player_cards)\n            current_player = current_player\n            \n            if cards_in_deck and n_cards_in_deck > 0:   \n                # random.shuffle(list(cards_in_deck))\n                card_in_play = random.choice(cards_in_deck)\n                n_cards_in_deck -= 1\n            else:\n                card_in_play = None\n            coins_in_play = 0\n\n        else:\n            coins[current_player] -= 1\n            coins_in_play += 1\n            current_player += 1\n        \n        if current_player == self.n_players:\n            current_player = 0\n\n        next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n        return self.pack_state(next_state)\n    \n    def all_possible_next(self, state, action):\n        if action == ACTION_PASS:\n            return self.next_state(state, action)\n        elif action == ACTION_TAKE:\n            next_states = []\n            state = self.unpack_state(state)\n            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n            cards[current_player].append(card_in_play)\n            coins[current_player] += coins_in_play\n            n_cards_in_deck -= 1\n            coins_in_play = 0\n\n            all_player_cards = [card for player_cards in cards for card in player_cards]\n            cards_in_deck = diff(self.full_deck, all_player_cards)\n            current_player = current_player\n            \n            if not cards_in_deck:\n                return self.next_state(state, action)\n            else:\n                for card in cards_in_deck: \n                    card_in_play = card\n                    if current_player == self.n_players:\n                        current_player = 0\n                    next_state = coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player)\n                    next_state = self.pack_state(next_state)\n                    next_states.append(next_state)\n            \n            return next_states\n\n    def is_legal(self, state, action):\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if card_in_play is None:\n            return False\n        if coins[current_player] <= 0 and action == ACTION_PASS:\n            return False\n        else:\n            return True\n\n    def legal_actions(self, state):\n        actions = []\n        \n        if self.is_legal(state, ACTION_TAKE):\n            actions.append(ACTION_TAKE)\n\n        if self.is_legal(state, ACTION_PASS):\n            actions.append(ACTION_PASS)\n\n        return actions\n\n    def pack_state(self, state):\n        coins, cards, details = state\n        packed_state = tuple(coins), tuple(map(tuple, cards)), details\n        return packed_state\n\n    def unpack_state(self, packed_state):\n        coins, cards, details = packed_state\n        coins = list(coins)\n        cards = list(map(list, cards))\n        return coins, cards, details\n    \n\n    def standard_state(self, state):\n        \"\"\"\n        Input state (packed or unpacked): ([coins], [[cards]], (card_in_play, coins_in_play, n_cards_in_deck, current_player))\n        Transform state into the required format:\n        1. Extract the state into M and b where M is the card/coin matrix and b is the vector (card_in_play, coins_in_play, n_cards_in_deck)\n        2. Rotate M such that the first row corresponds to the current player\n        3. Transform M into array of shape (3, n_players, 33)\n        \"\"\"\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n        \n        # Step 1: Build the M matrix (n_players x 34)\n        M = []\n        for k in range(self.n_players):\n            # Initialize the card representation for player k\n            card_rep = [0] * self.n_cards  # 33 cards\n\n            # Set 1 for each card player k has\n            for card in cards[k]:\n                card_rep[card - self.min_card] = 1  # Cards are indexed from 3 to 35\n\n            # Add the number of coins for player k\n            M.append(card_rep + [coins[k]])  # 33 card columns + 1 coin column\n        \n        # Step 2: Build the b vector (card_in_play, coins_in_play, n_cards_in_deck)\n        b = np.array([card_in_play, coins_in_play, n_cards_in_deck])\n\n        # Step 3: Rotate M such that the first row is the current player\n        M_rotated = M[current_player:] + M[:current_player]  # Rotate the matrix\n        \n        # Step 4: Transform M into array of shape (3, n_players, 33)\n        # where M[0] is the card matrix, M[1] is the coin matrix, M[2] is the card in play\n        M_transformed = np.zeros((3, self.n_players, self.n_cards))\n        M_rotated = np.array(M_rotated)\n        M_transformed[0] = M_rotated[:, :-1]  # Card matrix\n        M_transformed[1] = np.repeat(M_rotated[:, -1][:, np.newaxis], self.n_cards, axis=1)  # Coin matrix\n        M_transformed[1] = M_transformed[1] / (self.start_coins * self.n_players)  # Normalize coins to be between 0 and 1\n        \n        card_in_play_onehot = np.zeros(self.n_cards)\n        if self.min_card <= card_in_play <= self.max_card:\n            card_in_play_onehot[card_in_play - self.min_card] = 1\n        M_transformed[2] = np.tile(card_in_play_onehot, (self.n_players, 1))\n\n        return M_transformed, b\n\n\n    def is_ended(self, state):\n        # print(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if n_cards_in_deck == 0 and card_in_play == None:\n            return True\n        else:\n            return False\n\n    def compute_scores(self, state):\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        scores = []\n\n        for p_idx in range(self.n_players):\n            cards[p_idx].sort()\n\n            score = 0\n            if cards[p_idx]:\n                score += cards[p_idx][0]\n                last_card = cards[p_idx][0]\n\n                for card_idx in range(1, len(cards[p_idx])):\n                    new_card = cards[p_idx][card_idx]\n\n                    if not new_card == last_card + 1:\n                        score += new_card\n                    last_card = new_card\n\n            score -= coins[p_idx]\n\n            scores.append(score)\n\n        return scores\n\n    def winner(self, state):\n        \"\"\"Temporary winner: player with the lowest score even if the game is not ended.\"\"\"\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        if not self.is_ended(state):\n            return None\n        \n        scores = self.compute_scores(state)\n        min_score = 1000\n        lowest_scorers = []\n        # get lowest scorers (could be more than one)\n        for i, score in enumerate(scores):\n            if score < min_score:\n                lowest_scorers = [i]\n                min_score = score\n            if score <= min_score:\n                lowest_scorers.append(i)\n        \n        # if players are tied on lowest score, get the one with the fewest cards\n        if len(lowest_scorers) > 1:\n            min_n_cards = 1000\n            for i in lowest_scorers:\n                n_cards = len(cards[i])\n                if n_cards < min_n_cards:\n                    lowest_card_players = [i]\n                    min_n_cards = n_cards\n                elif n_cards <= min_n_cards:\n                    lowest_card_players.append(i)\n\n            if len(lowest_card_players) > 1:\n                winner = lowest_card_players[0]\n            else: # if still tied, pick a random winner (not the official rules)\n                winner = random.choice(lowest_card_players) \n        else:\n            winner = lowest_scorers[0]\n\n        return winner\n\n    \n    def basic_display_state(self, state):\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        print(\"Coins:           {0}\".format(coins))\n        print(\"Cards:           {0}\".format(cards))\n        print(\"Card in play:    {0}\".format(card_in_play))\n        print(\"Coins:           {0}\".format(coins_in_play))\n        print(\"Player:          {0}\".format(current_player))\n\n    def display_scores(self, state, players):\n        scores = self.compute_scores(state)\n        print(\"\")\n        print(\"--- Scores ---\")\n        for player in players:\n            print(\"{:<10} {:<10}\".format(\n                player.name, scores[player.turn])\n            )\n        print(\"\")\n\n    def display_state(self, state, players):\n        state = self.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state\n\n        scores = self.compute_scores(state)\n\n        def format_cards(card_list):\n            return \", \".join(map(str, sorted(card_list)))\n\n        player_labels = [player.name for player in players]\n        card_strings = [format_cards(cards[i]) for i in range(self.n_players)]\n        coin_strings = [str(coins[i]) for i in range(self.n_players)]\n        score_strings = [str(scores[i]) for i in range(self.n_players)]\n\n        max_card_len = max(20, max(len(card_str) for card_str in card_strings))\n\n        print(\"\")\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n        print(\"\")\n        print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\"Player\", \"Cards\", max_card_len, \"Coins\", \"Score\"))\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n\n        for i in range(self.n_players):\n            print(\"{:<15} {:<{}} {:<10} {:<10}\".format(\n                player_labels[i],\n                card_strings[i],\n                max_card_len,\n                coin_strings[i],\n                score_strings[i]\n            ))\n\n        print(\"-\" * (15 + max_card_len + 10 + 10 + 10))\n        print(\"\\t\\t In play: [{0}]\".format(card_in_play))\n        print(\"\\t\\t Cards remaining: {0}\".format(n_cards_in_deck))\n        print(\"\\t\\t   Coins: {0}\".format(coins_in_play))\n        print(\"\")\n            \n    def pack_action(self, notation):\n        if notation == \"y\" or notation == \"Y\":\n            return ACTION_TAKE\n        else:\n            return ACTION_PASS\n\n    def current_player(self, state):\n        return state[2][3]\n    \n    def remaining_cards(self, state):\n        s = self.unpack(state)\n\n        # All cards in original deck\n        full_deck = set(self.initial_deck)  # This should be fixed when game starts\n\n        # Cards already taken by players\n        taken_cards = set()\n        for player_cards in s[\"player_cards\"]:\n            taken_cards.update(player_cards)\n\n        # Card currently in play\n        if s[\"card\"] is not None:\n            taken_cards.add(s[\"card\"])\n\n        # Cards already revealed (optional: e.g., discard pile if any)\n        # taken_cards.update(s.get(\"discarded_cards\", []))\n\n        # Remaining = full_deck - taken\n        remaining = full_deck - taken_cards\n        return list(remaining)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:00.954700Z","iopub.execute_input":"2025-04-15T19:05:00.955034Z","iopub.status.idle":"2025-04-15T19:05:00.996092Z","shell.execute_reply.started":"2025-04-15T19:05:00.955011Z","shell.execute_reply":"2025-04-15T19:05:00.995028Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"## Players - UCT and PUCT Players [Open Loop MCTS]\n\nclass Player(ABC):\n    \"\"\"The abstract class for a player. A player can be an AI agent (bot) or human.\"\"\"\n    def __init__(self, game, turn):\n        self.name = \"Player \" + str(turn)\n        self.game = game\n        self.turn = turn  # starting form 0 as convention in python\n        assert self.turn < self.game.n_players, \"Player turn out of range.\"\n\n    @abstractmethod\n    def get_action(self, state):\n        pass\n\n\nclass BaseMCTSPlayer(Player, ABC):\n    def __init__(self, game, turn, thinking_time=1, simNum=0, max_moves=200):\n        super().__init__(game, turn)\n        self.thinking_time = thinking_time\n        self.simNum = simNum\n        self.max_moves = max_moves\n        self.max_depth = 0\n\n    @abstractmethod\n    def get_action(self, state):\n        pass\n\n    def score(self, state, player, legal_actions, plays, wins):\n        total_ply = sum(plays[(\"decision\", player, state, a)] for a in legal_actions)\n        if total_ply == 0:\n            return 0\n        score = 0\n        for action in legal_actions:\n            key = (\"decision\", player, state, action)\n            if plays[key]:\n                score += (wins[key] / plays[key]) * (plays[key] / total_ply)\n        return score\n\n\nclass UCTPlayer(BaseMCTSPlayer):\n    def __init__(self, game, turn=0, thinking_time=1, simNum=0):\n        super().__init__(game, turn, thinking_time, simNum)\n        self.C = 1.4  # Exploration parameter\n\n    def get_action(self, state):\n        board = self.game\n        player = board.current_player(state)\n        legal_actions = board.legal_actions(state)\n\n        if not legal_actions:\n            return None, None\n        if len(legal_actions) == 1:\n            return legal_actions[0], 0\n\n        plays = defaultdict(int)\n        wins = defaultdict(int)\n        games = 0\n\n        if self.thinking_time > 0 and self.simNum == 0:\n            start_time = time.perf_counter()\n            while time.perf_counter() - start_time < self.thinking_time:\n                self.run_simulation(state, board, plays, wins)\n                games += 1\n        else:\n            for _ in range(self.simNum):\n                self.run_simulation(state, board, plays, wins)\n                games += 1\n\n        random.shuffle(legal_actions)\n        action = max(\n            legal_actions,\n            key=lambda a: plays.get((player, state, a), 0)\n        )\n\n        return action, self.score(state, player, legal_actions, plays, wins)\n\n    def run_simulation(self, state, board, plays, wins):\n        \"\"\"Run a single MCTS simulation.\"\"\"\n        tree = set()\n        player = board.current_player(state)\n\n        # === Selection & Expansion ===\n        for t in range(1, self.max_moves + 1):\n            legal_actions = board.legal_actions(state)\n\n            # Selection using UCB1 if data exists for all actions\n            if all(plays.get((player, state, a)) for a in legal_actions):\n                log_total = log(sum(plays[(player, state, a)] for a in legal_actions))\n                action = max(\n                    legal_actions,\n                    key=lambda a: (\n                        wins[(player, state, a)] / plays[(player, state, a)] +\n                        self.C * sqrt(log_total / plays[(player, state, a)])\n                    )\n                )\n                \n            else:\n                # Expansion – If any action is unexplored, take a random one\n                action = random.choice(legal_actions)\n                if (player, state, action) not in plays:\n                    plays[(player, state, action)] = 0\n                    wins[(player, state, action)] = 0\n                    if t > self.max_depth:\n                        self.max_depth = t\n\n            tree.add((player, state, action))\n            state = board.next_state(state, action)\n            player = board.current_player(state)\n\n            # Check for game-ending state\n            winner = board.winner(state)\n            if winner is not None:\n                break\n\n        # === Backpropagation ===\n        for player, state, action in tree:\n            plays[(player, state, action)] += 1\n            if player == winner:\n                wins[(player, state, action)] += 1\n\n\nclass PUCTPlayer(BaseMCTSPlayer):\n    def __init__(self, game, turn=0, thinking_time=1, simNum=0):\n        super().__init__(game, turn, thinking_time, simNum)\n        self.C = 1.5 # c_puct exploration parameter\n        self.prior = lambda state, action: 1 / len(self.game.legal_actions(state))\n        self.value = None\n\n    def get_action(self, state):\n        board = self.game\n        player = board.current_player(state)\n        legal_actions = board.legal_actions(state)\n\n        if not legal_actions:\n            return None, None\n        if len(legal_actions) == 1:\n            return legal_actions[0], 0\n\n        plays = defaultdict(int)\n        wins = defaultdict(int)\n        games = 0\n\n        if self.thinking_time > 0 and self.simNum == 0:\n            start_time = time.perf_counter()\n            while time.perf_counter() - start_time < self.thinking_time:\n                self.run_simulation(state, board, plays, wins)\n                games += 1\n        else:\n            for _ in range(self.simNum):\n                self.run_simulation(state, board, plays, wins)\n                games += 1\n\n        random.shuffle(legal_actions)\n        action = max(\n            legal_actions,\n            key=lambda a: plays.get((player, state, a), 0)\n        )\n\n        return action, self.score(state, player, legal_actions, plays, wins)\n\n    def run_simulation(self, state, board, plays, wins):\n        \"\"\"Run a single MCTS simulation.\"\"\"\n        tree = set()\n        player = board.current_player(state)\n\n        # === Selection & Expansion ===\n        for t in range(1, self.max_moves + 1):\n            legal_actions = board.legal_actions(state)\n\n            # Selection using UCB1 if data exists for all actions\n            if all(plays.get((player, state, a)) for a in legal_actions):\n                total = sum(plays[(player, state, a)] for a in legal_actions)\n                # for a in legal_actions:\n                #     print(\"Check Total:\", a, wins[(player, state, a)] / plays[(player, state, a)])\n                action = max(\n                    legal_actions,\n                    key=lambda a: (\n                        wins[(player, state, a)] / plays[(player, state, a)] +\n                        self.C * self.prior(state, a) * sqrt(total / plays[(player, state, a)])\n                    )\n                )\n            else:\n                # Expansion – If any action is unexplored, take a random one\n                action = random.choice(legal_actions)\n                if (player, state, action) not in plays:\n                    plays[(player, state, action)] = 0\n                    wins[(player, state, action)] = 0\n                    if t > self.max_depth:\n                        self.max_depth = t\n\n            tree.add((player, state, action)) # trajectory\n            state = board.next_state(state, action)\n            player = board.current_player(state)\n\n            # Check for game-ending state\n            winner = board.winner(state)\n            if winner is not None:\n                break\n\n        # === Backpropagation ===\n        for player, state, action in tree:\n            plays[(player, state, action)] += 1\n            if player == winner:\n                wins[(player, state, action)] += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:00.997175Z","iopub.execute_input":"2025-04-15T19:05:00.997524Z","iopub.status.idle":"2025-04-15T19:05:01.028785Z","shell.execute_reply.started":"2025-04-15T19:05:00.997492Z","shell.execute_reply":"2025-04-15T19:05:01.027721Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"## Utils - prior functions\n\ndef nn_prior_fn(model, game):\n    def nn_prior(state, action):\n        with torch.no_grad():\n            M, b = game.standard_state(state)\n            M_tensor = torch.tensor(M, dtype=torch.float32).unsqueeze(0)\n            b_tensor = torch.tensor(b, dtype=torch.float32).unsqueeze(0)\n            policy = model(M_tensor, b_tensor)[0]\n            prob = policy.item()\n            return (prob if action == ACTION_PASS else 1 - prob)\n    return nn_prior\n\ndef smart_prior_fn(game, p=0.98):\n    def smart_prior(state, action):\n        state = game.unpack_state(state)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = state \n        other_cards = [i for i in list(chain.from_iterable(cards)) if i not in cards[current_player]]\n        good_for_me = any(abs(card_in_play - card) < 2 for card in cards[current_player])\n        good_for_them = any(abs(card_in_play - card) < 2 for card in other_cards)\n        least_chip = min(coins)\n        legal_actions = game.legal_actions(state)\n\n        if good_for_me:\n            if good_for_them:\n                good_action = ACTION_TAKE\n            else:\n                good_action = ACTION_TAKE\n                if least_chip > 2:\n                    good_action = ACTION_PASS\n        else:\n            good_action = ACTION_PASS\n            if coins[current_player] < 2 or abs(coins_in_play - card_in_play) < min(3, card_in_play//2):\n                good_action = ACTION_TAKE\n\n        if action not in legal_actions:\n            return 0  # Invalid action for the state\n        return p if action == good_action else (1 - p)\n    \n    return smart_prior\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:01.029822Z","iopub.execute_input":"2025-04-15T19:05:01.030132Z","iopub.status.idle":"2025-04-15T19:05:01.052651Z","shell.execute_reply.started":"2025-04-15T19:05:01.030110Z","shell.execute_reply":"2025-04-15T19:05:01.051662Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"## Utils - play and performance evaluation\n\ndef play(game, players, display=True):    \n    players.sort(key=lambda x: x.turn)\n    current_player = players[0].turn\n    \n    state = game.starting_state(current_player=current_player)\n    state = game.pack_state(state)\n\n    while not game.is_ended(state):\n        player = players[current_player]\n        action, score = player.get_action(state)\n        state = game.next_state(state, action)\n        coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = game.unpack_state(state)\n        if display:\n            game.display_state(state, players)\n        \n        # print(game.standard_state(state))  \n    winner = game.winner(state)\n    if display:\n        game.display_scores(state, players)\n        print(\"Game ended. Player\", winner, \"wins!\")\n\n    return winner\n\ndef eval_performance(game, target_player, opponents, num_games=100, verbose=False):\n    random.seed(time.time())\n    target_player.name = \"Target\"\n    players = [target_player] + opponents\n    win = defaultdict(int)\n    for i in bar(range(num_games)):\n        target_player.turn = i % len(players)\n        for j, player in enumerate(players):\n            if player != target_player:\n                player.turn = (i + j) % len(players)\n            # print(f\"Game {i+1}: Player {player.name} turn: {player.turn}\")\n        winner = play(game, players, display=False)\n        win[players[winner].name] += 1\n        if verbose and i in [150, 180, 210, 240, 270]:\n            print(f\"Number of wins for each player: {win}\")\n\n    print(f\"Number of wins for each player: {win}\")\n    winrate = win[target_player.name] / num_games\n\n    return winrate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:01.053830Z","iopub.execute_input":"2025-04-15T19:05:01.054205Z","iopub.status.idle":"2025-04-15T19:05:01.081237Z","shell.execute_reply.started":"2025-04-15T19:05:01.054176Z","shell.execute_reply":"2025-04-15T19:05:01.080153Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"## Training process functions\n\ndef self_play(game, players, times=1, to_file=None, smart=False):\n    if smart:\n        for player in players:\n            player.prior = smart_prior_fn(game)\n\n    data = {\"state\": [], \"policy\": [], \"value\": []}\n    for _ in bar(range(times)):\n        state = game.starting_state(current_player=0)\n        state = game.pack_state(state)\n        current_player = 0\n\n        while not game.is_ended(state):\n            player = players[current_player]\n            action, score = player.get_action(state)\n            state = game.next_state(state, action)\n            coins, cards, (card_in_play, coins_in_play, n_cards_in_deck, current_player) = game.unpack_state(state)\n            if card_in_play is not None:\n                data[\"state\"].append(game.standard_state(state))  # Append (M, b) as NumPy arrays\n                data[\"policy\"].append(action)\n                data[\"value\"].append(score)\n\n    # Convert NumPy arrays in \"state\" to lists for JSON serialization\n    if to_file is not None:\n        serializable_data = {\n            \"state\": [(M.tolist(), b.tolist()) for M, b in data[\"state\"]],\n            \"policy\": data[\"policy\"],\n            \"value\": data[\"value\"]\n        }\n        with open(to_file, \"w\") as f:\n            json.dump(serializable_data, f)\n\n    return data\n\ndef rl_train(rounds=10, num_games=4, simNum=1000, prior=False, ctd_from=0):\n    game = NoThanksBoard(n_players=3)\n    Player_0 = PUCTPlayer(game=game, turn=0)\n    Player_1 = PUCTPlayer(game=game, turn=1)\n    Player_2 = PUCTPlayer(game=game, turn=2)\n    players = [Player_0, Player_1, Player_2]\n\n    tester = PUCTPlayer(game=game, turn=0, simNum=simNum)\n\n    batch_size = 32\n    n_players = 3\n    model = PolicyValueNet(n_players, hidden_dim=32)\n\n    if prior == True:\n        model.load_state_dict(torch.load(f'policy_value_net_rd{ctd_from}.pth', weights_only=True))\n        model.eval()\n        for player in players:\n            player.prior = nn_prior_fn(model, game)\n            player.simNum = simNum\n        print(f\"Model {ctd_from} loaded...Priors are updated for players.\")\n\n    for i in range(rounds):\n        print(f\"Round {i}: The bots are playing...\")\n\n        # smart = True if i == 0 else False\n        smart = False\n        data = self_play(game, players, times=num_games, smart=smart)\n        \n        # Combine and shuffle the data\n        combined_data = list(zip(data[\"state\"], data[\"policy\"], data[\"value\"]))\n        random.shuffle(combined_data)\n        data[\"state\"], data[\"policy\"], data[\"value\"] = zip(*combined_data)\n\n        states = data[\"state\"]\n        target_policy = np.array(data[\"policy\"])\n        target_value = np.array(data[\"value\"])\n        print(\"Training Data prepared.\")\n        num_samples = len(states)\n        num_batches = num_samples // batch_size\n        print(f\"Total samples: {num_samples}, Batches: {num_batches}\")\n\n        backup_model = copy.deepcopy(model.state_dict())\n            \n        for batch_idx in range(num_batches):\n            # Extract batch data\n            batch_start = batch_idx * batch_size\n            batch_end = batch_start + batch_size\n            \n            batch_states = states[batch_start:batch_end]\n            batch_policies = target_policy[batch_start:batch_end]\n            batch_values = target_value[batch_start:batch_end]\n            \n            # Reshape states into (M, b) format\n            M = np.array([s[0] for s in batch_states])\n            b = np.array([np.array(s[1], dtype=np.float32) for s in batch_states], dtype=np.float32)\n\n            # Train the model on the batch\n            model.train()\n            print(f\"Training batch {batch_idx + 1}/{num_batches}\")\n            train_nn(model, batch_size, n_players, 32, (M, b), batch_policies, batch_values)\n\n        # Save the model\n        torch.save(model.state_dict(), f'policy_value_net_rd{i+ctd_from}.pth')\n\n        print(f\"Round {i} completed. Evaluating performance...\")\n        # for the first round, the players have thinking time 1 sec (~15000 rollouts); \n        # after that the players have simNum rollouts (including evaluation) for speeding-up.\n        if i == 0:\n            for player in players:\n                player.simNum = simNum\n        \n        # Update prior for the tester\n        model.eval()\n        tester.prior = nn_prior_fn(model, game)\n\n        # Evaluate the performance of the trained model\n        # need about 300 games to reach stable estimate of winrate\n        winrate = eval_performance(game, tester, players[1:], num_games=300, verbose=False)\n        if winrate > 0.38:\n            print(f\"Winrate of the Target Player: {winrate:.2%}; Model is accepted.\")\n            for player in players:\n                player.prior = nn_prior_fn(model, game)\n        else:\n            print(f\"Winrate of the Target Player: {winrate:.2%}; Model is rejected.\")\n            model.load_state_dict(backup_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:01.082261Z","iopub.execute_input":"2025-04-15T19:05:01.082671Z","iopub.status.idle":"2025-04-15T19:05:01.109212Z","shell.execute_reply.started":"2025-04-15T19:05:01.082645Z","shell.execute_reply":"2025-04-15T19:05:01.108122Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"## Training\n\nrl_train(rounds=10, num_games=300, simNum=500, prior=False, ctd_from=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:05:01.110083Z","iopub.execute_input":"2025-04-15T19:05:01.110341Z"}},"outputs":[{"name":"stdout","text":"Round 0: The bots are playing...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2/2 [01:14<00:00, 37.29s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training Data prepared.\nTotal samples: 80, Batches: 2\nTraining batch 1/2\nLoss = 0.7684, Policy Loss = 0.7178, Value Loss = 0.2134\nTraining batch 2/2\nLoss = 0.7392, Policy Loss = 0.6888, Value Loss = 0.1988\nRound 0 completed. Evaluating performance...\n","output_type":"stream"},{"name":"stderr","text":" 19%|█▉        | 57/300 [35:37<2:31:40, 37.45s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ## Evaluate Perfromance\n\n# game = NoThanksBoard(n_players = 3)\n# Player_0 = PUCTPlayer(game=game, turn=0, simNum=500)\n# Player_1 = PUCTPlayer(game=game, turn=1, simNum=500)\n# Player_2 = PUCTPlayer(game, turn=2, simNum=500)\n\n# # model = PolicyValueNet(game.n_players, 32)\n# # model.load_state_dict(torch.load('policy_value_net_rd0.pth', weights_only=True))\n# # model.eval()\n\n# # model = PolicyOnlyNet(game.n_players, 128)\n# # model.load_state_dict(torch.load('policy_only_net.pth'))\n# # model.eval()\n\n# # new_prior = smart_prior_fn(game)\n# # new_prior = nn_prior_fn(model, game)\n# # Player_0.prior = smart_prior_fn(game)\n# # Player_1.prior = new_prior\n# # Player_2.prior = new_prior\n\n# players = [Player_0, Player_1, Player_2]\n\n# # rl_train(rounds=1, num_games=2, simNum=2000, prior=False, ctd_from=0)\n\n# # play(game, players, display=True)\n\n# winrate = eval_performance(game, Player_0, [Player_1, Player_2], num_games=300, verbose=True)\n\n# print(f\"Winrate of the Target Player: {winrate:.2%}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}